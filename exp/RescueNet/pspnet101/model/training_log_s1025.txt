[2025-02-11 03:53:30,111] INFO: arch: pspnet
aux_weight: 0.4
base_lr: 0.001
base_size: 2048
batch_size: 2
batch_size_val: 1
classes: 11
data_root: dataset/RescueNet
dataset: rescuenet
dataset_dir: dataset/RescueNet
device: cuda
dist_backend: nccl
dist_url: tcp://127.0.0.1:6789
distributed: False
epochs: 5
evaluate: False
has_prediction: False
ignore_label: 255P
ignore_unlabeled: True
imshow_batch: True
index_split: 5
index_start: 0
index_step: 0
keep_batchnorm_fp32: None
layers: 101
loss_scale: None
manual_seed: None
mode: vis
model_path: None
momentum: 0.9
multiprocessing_distributed: False
ngpus_per_node: 1
opt_level: O0
output: outputs
power: 0.9
predict_color: True
print_freq: 10
print_step: False
rank: 0
resume: None
rotate_max: 10
rotate_min: -10
save_folder: None
save_freq: 1
save_path: exp/RescueNet/pspnet101/model
scale_max: 2.0
scale_min: 0.5
scales: [1.0]
split: val
start_epoch: 0
sync_bn: False
test_gpu: [0]
test_h: 713
test_list: dataset/michael/list/test_img_list.txt
test_w: 713
train_gpu: [0]
train_h: 1025
train_list: dataset/cityscapes/list/fine_train.txt
train_w: 1025
use_apex: True
use_pretrained_weights: True
val_list: dataset/cityscapes/list/fine_val.txt
weight: None
weight_decay: 1e-05
workers: 4
world_size: 1
zoom_factor: 8
[2025-02-11 03:53:30,111] INFO: => creating model ...
[2025-02-11 03:53:30,112] INFO: Classes: 11
[2025-02-11 03:53:30,112] INFO: PSPNet(
  (criterion): CrossEntropyLoss()
  (layer0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (6): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (7): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (8): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (9): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (10): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (11): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (12): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (13): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (14): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (15): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (16): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (17): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (18): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (19): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (20): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (21): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (22): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (ppm): PPM(
    (features): ModuleList(
      (0): Sequential(
        (0): AdaptiveAvgPool2d(output_size=1)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): AdaptiveAvgPool2d(output_size=2)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): AdaptiveAvgPool2d(output_size=3)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (3): Sequential(
        (0): AdaptiveAvgPool2d(output_size=6)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
    )
  )
  (cls): Sequential(
    (0): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Dropout2d(p=0.1, inplace=False)
    (4): Conv2d(512, 11, kernel_size=(1, 1), stride=(1, 1))
  )
  (aux): Sequential(
    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Dropout2d(p=0.1, inplace=False)
    (4): Conv2d(256, 11, kernel_size=(1, 1), stride=(1, 1))
  )
)
[2025-02-11 05:14:52,030] INFO: arch: pspnet
aux_weight: 0.4
base_lr: 0.001
base_size: 2048
batch_size: 2
batch_size_val: 1
classes: 11
data_root: dataset/RescueNet
dataset: rescuenet
dataset_dir: dataset/RescueNet
device: cuda
dist_backend: nccl
dist_url: tcp://127.0.0.1:6789
distributed: False
epochs: 5
evaluate: False
has_prediction: False
ignore_label: 255P
ignore_unlabeled: True
imshow_batch: True
index_split: 5
index_start: 0
index_step: 0
keep_batchnorm_fp32: None
layers: 101
loss_scale: None
manual_seed: None
mode: vis
model_path: None
momentum: 0.9
multiprocessing_distributed: False
ngpus_per_node: 1
opt_level: O0
output: outputs
power: 0.9
predict_color: True
print_freq: 10
print_step: False
rank: 0
resume: None
rotate_max: 10
rotate_min: -10
save_folder: None
save_freq: 1
save_path: exp/RescueNet/pspnet101/model
scale_max: 2.0
scale_min: 0.5
scales: [1.0]
split: val
start_epoch: 0
sync_bn: False
test_gpu: [0]
test_h: 713
test_list: dataset/michael/list/test_img_list.txt
test_w: 713
train_gpu: [0]
train_h: 1025
train_list: dataset/cityscapes/list/fine_train.txt
train_w: 1025
use_apex: True
use_pretrained_weights: True
val_list: dataset/cityscapes/list/fine_val.txt
weight: None
weight_decay: 1e-05
workers: 4
world_size: 1
zoom_factor: 8
[2025-02-11 05:14:52,031] INFO: => creating model ...
[2025-02-11 05:14:52,032] INFO: Classes: 11
[2025-02-11 05:14:52,032] INFO: PSPNet(
  (criterion): CrossEntropyLoss()
  (layer0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (6): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (7): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (8): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (9): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (10): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (11): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (12): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (13): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (14): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (15): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (16): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (17): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (18): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (19): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (20): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (21): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (22): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (ppm): PPM(
    (features): ModuleList(
      (0): Sequential(
        (0): AdaptiveAvgPool2d(output_size=1)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): AdaptiveAvgPool2d(output_size=2)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): AdaptiveAvgPool2d(output_size=3)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (3): Sequential(
        (0): AdaptiveAvgPool2d(output_size=6)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
    )
  )
  (cls): Sequential(
    (0): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Dropout2d(p=0.1, inplace=False)
    (4): Conv2d(512, 11, kernel_size=(1, 1), stride=(1, 1))
  )
  (aux): Sequential(
    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Dropout2d(p=0.1, inplace=False)
    (4): Conv2d(256, 11, kernel_size=(1, 1), stride=(1, 1))
  )
)
[2025-02-11 05:23:45,187] INFO: arch: pspnet
aux_weight: 0.4
base_lr: 0.001
base_size: 2048
batch_size: 2
batch_size_val: 1
classes: 11
data_root: dataset/RescueNet
dataset: rescuenet
dataset_dir: dataset/RescueNet
device: cuda
dist_backend: nccl
dist_url: tcp://127.0.0.1:6789
distributed: False
epochs: 5
evaluate: False
has_prediction: False
ignore_label: 255
ignore_unlabeled: True
imshow_batch: True
index_split: 5
index_start: 0
index_step: 0
keep_batchnorm_fp32: None
layers: 101
loss_scale: None
manual_seed: None
mode: vis
model_path: None
momentum: 0.9
multiprocessing_distributed: False
ngpus_per_node: 1
opt_level: O0
output: outputs
power: 0.9
predict_color: True
print_freq: 10
print_step: False
rank: 0
resume: None
rotate_max: 10
rotate_min: -10
save_folder: None
save_freq: 1
save_path: exp/RescueNet/pspnet101/model
scale_max: 2.0
scale_min: 0.5
scales: [1.0]
split: val
start_epoch: 0
sync_bn: False
test_gpu: [0]
test_h: 713
test_list: dataset/michael/list/test_img_list.txt
test_w: 713
train_gpu: [0]
train_h: 1025
train_list: dataset/cityscapes/list/fine_train.txt
train_w: 1025
use_apex: True
use_pretrained_weights: True
val_list: dataset/cityscapes/list/fine_val.txt
weight: None
weight_decay: 1e-05
workers: 4
world_size: 1
zoom_factor: 8
[2025-02-11 05:23:45,188] INFO: => creating model ...
[2025-02-11 05:23:45,188] INFO: Classes: 11
[2025-02-11 05:23:45,189] INFO: PSPNet(
  (criterion): CrossEntropyLoss()
  (layer0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (6): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (7): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (8): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (9): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (10): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (11): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (12): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (13): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (14): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (15): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (16): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (17): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (18): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (19): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (20): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (21): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (22): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (ppm): PPM(
    (features): ModuleList(
      (0): Sequential(
        (0): AdaptiveAvgPool2d(output_size=1)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): AdaptiveAvgPool2d(output_size=2)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): AdaptiveAvgPool2d(output_size=3)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (3): Sequential(
        (0): AdaptiveAvgPool2d(output_size=6)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
    )
  )
  (cls): Sequential(
    (0): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Dropout2d(p=0.1, inplace=False)
    (4): Conv2d(512, 11, kernel_size=(1, 1), stride=(1, 1))
  )
  (aux): Sequential(
    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Dropout2d(p=0.1, inplace=False)
    (4): Conv2d(256, 11, kernel_size=(1, 1), stride=(1, 1))
  )
)
[2025-02-11 05:23:51,501] INFO: Epoch: [1/5][10/1797] Data 0.006 (0.040) Batch 0.540 (0.616) Remain 01:32:04 MainLoss 1.9692 AuxLoss 1.7952 Loss 2.6873 Accuracy 0.3438.
[2025-02-11 05:23:56,915] INFO: Epoch: [1/5][20/1797] Data 0.006 (0.023) Batch 0.541 (0.579) Remain 01:26:26 MainLoss 1.3150 AuxLoss 1.4054 Loss 1.8771 Accuracy 0.5497.
[2025-02-11 05:24:02,320] INFO: Epoch: [1/5][30/1797] Data 0.006 (0.018) Batch 0.539 (0.566) Remain 01:24:27 MainLoss 1.4594 AuxLoss 1.3234 Loss 1.9887 Accuracy 0.3804.
[2025-02-11 05:24:07,724] INFO: Epoch: [1/5][40/1797] Data 0.006 (0.015) Batch 0.541 (0.559) Remain 01:23:24 MainLoss 2.1995 AuxLoss 1.1895 Loss 2.6753 Accuracy 0.4623.
[2025-02-11 05:24:13,133] INFO: Epoch: [1/5][50/1797] Data 0.006 (0.013) Batch 0.541 (0.556) Remain 01:22:45 MainLoss 1.1607 AuxLoss 0.8951 Loss 1.5187 Accuracy 0.7330.
[2025-02-11 05:24:18,549] INFO: Epoch: [1/5][60/1797] Data 0.006 (0.012) Batch 0.543 (0.553) Remain 01:22:19 MainLoss 0.6249 AuxLoss 1.0924 Loss 1.0618 Accuracy 0.8345.
[2025-02-11 05:24:23,961] INFO: Epoch: [1/5][70/1797] Data 0.006 (0.011) Batch 0.541 (0.552) Remain 01:21:58 MainLoss 1.5782 AuxLoss 1.4729 Loss 2.1673 Accuracy 0.4509.
[2025-02-11 05:24:29,376] INFO: Epoch: [1/5][80/1797] Data 0.006 (0.010) Batch 0.542 (0.550) Remain 01:21:41 MainLoss 0.8656 AuxLoss 0.8254 Loss 1.1957 Accuracy 0.7223.
[2025-02-11 05:24:34,795] INFO: Epoch: [1/5][90/1797] Data 0.006 (0.010) Batch 0.543 (0.549) Remain 01:21:27 MainLoss 0.9337 AuxLoss 1.0588 Loss 1.3573 Accuracy 0.7122.
[2025-02-11 05:24:40,210] INFO: Epoch: [1/5][100/1797] Data 0.006 (0.009) Batch 0.542 (0.549) Remain 01:21:14 MainLoss 1.0292 AuxLoss 1.0533 Loss 1.4505 Accuracy 0.4862.
[2025-02-11 05:24:45,632] INFO: Epoch: [1/5][110/1797] Data 0.006 (0.009) Batch 0.542 (0.548) Remain 01:21:04 MainLoss 1.1557 AuxLoss 1.3252 Loss 1.6858 Accuracy 0.5918.
[2025-02-11 05:24:51,055] INFO: Epoch: [1/5][120/1797] Data 0.006 (0.009) Batch 0.542 (0.548) Remain 01:20:54 MainLoss 1.2517 AuxLoss 1.1502 Loss 1.7118 Accuracy 0.5549.
[2025-02-11 05:24:56,480] INFO: Epoch: [1/5][130/1797] Data 0.006 (0.009) Batch 0.544 (0.547) Remain 01:20:45 MainLoss 0.4761 AuxLoss 0.9641 Loss 0.8617 Accuracy 0.8788.
[2025-02-11 05:25:01,905] INFO: Epoch: [1/5][140/1797] Data 0.006 (0.008) Batch 0.542 (0.547) Remain 01:20:36 MainLoss 1.5685 AuxLoss 1.0756 Loss 1.9988 Accuracy 0.5007.
[2025-02-11 05:25:07,336] INFO: Epoch: [1/5][150/1797] Data 0.006 (0.008) Batch 0.543 (0.547) Remain 01:20:29 MainLoss 0.6519 AuxLoss 1.1392 Loss 1.1076 Accuracy 0.8032.
[2025-02-11 05:25:12,760] INFO: Epoch: [1/5][160/1797] Data 0.006 (0.008) Batch 0.542 (0.546) Remain 01:20:21 MainLoss 0.8424 AuxLoss 1.0874 Loss 1.2773 Accuracy 0.7456.
[2025-02-11 05:25:18,183] INFO: Epoch: [1/5][170/1797] Data 0.006 (0.008) Batch 0.543 (0.546) Remain 01:20:13 MainLoss 1.3533 AuxLoss 1.0157 Loss 1.7596 Accuracy 0.6049.
[2025-02-11 05:25:23,805] INFO: Epoch: [1/5][180/1797] Data 0.006 (0.009) Batch 0.543 (0.547) Remain 01:20:16 MainLoss 1.3776 AuxLoss 1.1143 Loss 1.8234 Accuracy 0.5161.
[2025-02-11 05:25:29,237] INFO: Epoch: [1/5][190/1797] Data 0.006 (0.009) Batch 0.545 (0.547) Remain 01:20:09 MainLoss 1.3031 AuxLoss 1.3513 Loss 1.8436 Accuracy 0.6422.
[2025-02-11 05:25:34,658] INFO: Epoch: [1/5][200/1797] Data 0.006 (0.009) Batch 0.542 (0.547) Remain 01:20:01 MainLoss 1.3486 AuxLoss 1.5154 Loss 1.9548 Accuracy 0.5247.
[2025-02-11 05:25:40,084] INFO: Epoch: [1/5][210/1797] Data 0.006 (0.008) Batch 0.544 (0.546) Remain 01:19:54 MainLoss 0.9120 AuxLoss 1.2086 Loss 1.3954 Accuracy 0.7655.
[2025-02-11 05:25:46,049] INFO: Epoch: [1/5][220/1797] Data 0.005 (0.011) Batch 0.542 (0.549) Remain 01:20:08 MainLoss 1.1867 AuxLoss 1.2847 Loss 1.7005 Accuracy 0.7520.
[2025-02-11 05:25:51,470] INFO: Epoch: [1/5][230/1797] Data 0.006 (0.011) Batch 0.542 (0.548) Remain 01:20:00 MainLoss 0.9551 AuxLoss 1.1298 Loss 1.4070 Accuracy 0.5601.
[2025-02-11 05:25:56,895] INFO: Epoch: [1/5][240/1797] Data 0.006 (0.010) Batch 0.543 (0.548) Remain 01:19:53 MainLoss 1.1322 AuxLoss 1.1236 Loss 1.5816 Accuracy 0.6099.
[2025-02-11 05:26:02,320] INFO: Epoch: [1/5][250/1797] Data 0.006 (0.010) Batch 0.542 (0.548) Remain 01:19:45 MainLoss 1.4799 AuxLoss 1.7630 Loss 2.1851 Accuracy 0.5502.
[2025-02-11 05:26:07,748] INFO: Epoch: [1/5][260/1797] Data 0.006 (0.010) Batch 0.541 (0.548) Remain 01:19:38 MainLoss 0.8126 AuxLoss 0.8867 Loss 1.1673 Accuracy 0.6845.
[2025-02-11 05:26:13,178] INFO: Epoch: [1/5][270/1797] Data 0.006 (0.010) Batch 0.544 (0.548) Remain 01:19:31 MainLoss 0.6571 AuxLoss 0.8924 Loss 1.0141 Accuracy 0.8176.
[2025-02-11 05:26:18,612] INFO: Epoch: [1/5][280/1797] Data 0.006 (0.010) Batch 0.543 (0.547) Remain 01:19:24 MainLoss 0.8170 AuxLoss 0.9250 Loss 1.1870 Accuracy 0.8099.
[2025-02-11 05:26:24,047] INFO: Epoch: [1/5][290/1797] Data 0.006 (0.010) Batch 0.543 (0.547) Remain 01:19:18 MainLoss 0.8967 AuxLoss 1.0403 Loss 1.3129 Accuracy 0.7602.
[2025-02-11 05:26:29,472] INFO: Epoch: [1/5][300/1797] Data 0.005 (0.009) Batch 0.542 (0.547) Remain 01:19:11 MainLoss 2.1715 AuxLoss 1.3343 Loss 2.7052 Accuracy 0.3649.
[2025-02-11 05:26:34,900] INFO: Epoch: [1/5][310/1797] Data 0.006 (0.009) Batch 0.542 (0.547) Remain 01:19:04 MainLoss 1.4928 AuxLoss 1.2224 Loss 1.9818 Accuracy 0.4125.
[2025-02-11 05:26:40,331] INFO: Epoch: [1/5][320/1797] Data 0.006 (0.009) Batch 0.543 (0.547) Remain 01:18:58 MainLoss 0.9677 AuxLoss 0.9873 Loss 1.3626 Accuracy 0.6540.
[2025-02-11 05:26:45,757] INFO: Epoch: [1/5][330/1797] Data 0.006 (0.009) Batch 0.542 (0.547) Remain 01:18:51 MainLoss 1.3620 AuxLoss 1.3955 Loss 1.9202 Accuracy 0.5193.
[2025-02-11 05:26:51,190] INFO: Epoch: [1/5][340/1797] Data 0.006 (0.009) Batch 0.542 (0.547) Remain 01:18:45 MainLoss 1.7117 AuxLoss 1.6093 Loss 2.3554 Accuracy 0.2876.
[2025-02-11 05:26:56,619] INFO: Epoch: [1/5][350/1797] Data 0.006 (0.009) Batch 0.541 (0.546) Remain 01:18:38 MainLoss 1.4734 AuxLoss 1.1574 Loss 1.9364 Accuracy 0.4346.
[2025-02-11 05:27:02,049] INFO: Epoch: [1/5][360/1797] Data 0.005 (0.009) Batch 0.544 (0.546) Remain 01:18:32 MainLoss 0.5093 AuxLoss 0.8019 Loss 0.8301 Accuracy 0.8791.
[2025-02-11 05:27:07,477] INFO: Epoch: [1/5][370/1797] Data 0.006 (0.009) Batch 0.543 (0.546) Remain 01:18:26 MainLoss 0.9011 AuxLoss 1.0257 Loss 1.3113 Accuracy 0.6674.
[2025-02-11 05:27:12,906] INFO: Epoch: [1/5][380/1797] Data 0.006 (0.009) Batch 0.544 (0.546) Remain 01:18:20 MainLoss 0.7687 AuxLoss 1.1635 Loss 1.2341 Accuracy 0.8400.
[2025-02-11 05:27:18,335] INFO: Epoch: [1/5][390/1797] Data 0.006 (0.009) Batch 0.543 (0.546) Remain 01:18:13 MainLoss 0.8859 AuxLoss 0.9838 Loss 1.2795 Accuracy 0.6985.
[2025-02-11 05:27:23,763] INFO: Epoch: [1/5][400/1797] Data 0.006 (0.009) Batch 0.544 (0.546) Remain 01:18:07 MainLoss 0.9732 AuxLoss 1.1353 Loss 1.4274 Accuracy 0.6617.
[2025-02-11 05:27:29,190] INFO: Epoch: [1/5][410/1797] Data 0.006 (0.008) Batch 0.543 (0.546) Remain 01:18:01 MainLoss 1.5220 AuxLoss 1.5583 Loss 2.1453 Accuracy 0.5218.
[2025-02-11 05:27:34,622] INFO: Epoch: [1/5][420/1797] Data 0.006 (0.008) Batch 0.542 (0.546) Remain 01:17:55 MainLoss 0.8101 AuxLoss 0.7707 Loss 1.1184 Accuracy 0.7162.
[2025-02-11 05:27:40,052] INFO: Epoch: [1/5][430/1797] Data 0.006 (0.008) Batch 0.544 (0.546) Remain 01:17:49 MainLoss 0.8797 AuxLoss 1.0054 Loss 1.2819 Accuracy 0.7684.
[2025-02-11 05:27:45,477] INFO: Epoch: [1/5][440/1797] Data 0.006 (0.008) Batch 0.543 (0.546) Remain 01:17:43 MainLoss 0.4614 AuxLoss 0.8309 Loss 0.7938 Accuracy 0.8651.
[2025-02-11 05:27:50,909] INFO: Epoch: [1/5][450/1797] Data 0.006 (0.008) Batch 0.544 (0.546) Remain 01:17:37 MainLoss 0.4510 AuxLoss 0.5415 Loss 0.6676 Accuracy 0.8812.
[2025-02-11 05:27:56,330] INFO: Epoch: [1/5][460/1797] Data 0.006 (0.008) Batch 0.541 (0.546) Remain 01:17:31 MainLoss 1.1053 AuxLoss 1.3784 Loss 1.6566 Accuracy 0.6534.
[2025-02-11 05:28:01,756] INFO: Epoch: [1/5][470/1797] Data 0.006 (0.008) Batch 0.543 (0.546) Remain 01:17:25 MainLoss 0.9124 AuxLoss 0.9179 Loss 1.2795 Accuracy 0.7395.
[2025-02-11 05:28:07,177] INFO: Epoch: [1/5][480/1797] Data 0.005 (0.008) Batch 0.541 (0.545) Remain 01:17:19 MainLoss 1.2454 AuxLoss 1.2591 Loss 1.7490 Accuracy 0.4916.
[2025-02-11 05:28:12,602] INFO: Epoch: [1/5][490/1797] Data 0.006 (0.008) Batch 0.542 (0.545) Remain 01:17:13 MainLoss 1.1904 AuxLoss 1.1278 Loss 1.6415 Accuracy 0.4986.
[2025-02-11 05:28:18,035] INFO: Epoch: [1/5][500/1797] Data 0.006 (0.008) Batch 0.541 (0.545) Remain 01:17:07 MainLoss 2.1345 AuxLoss 1.8054 Loss 2.8567 Accuracy 0.3607.
[2025-02-11 05:28:23,458] INFO: Epoch: [1/5][510/1797] Data 0.006 (0.008) Batch 0.542 (0.545) Remain 01:17:01 MainLoss 1.5612 AuxLoss 1.4878 Loss 2.1563 Accuracy 0.3975.
[2025-02-11 05:28:28,884] INFO: Epoch: [1/5][520/1797] Data 0.005 (0.008) Batch 0.543 (0.545) Remain 01:16:55 MainLoss 0.8530 AuxLoss 1.1025 Loss 1.2940 Accuracy 0.6991.
[2025-02-11 05:28:34,311] INFO: Epoch: [1/5][530/1797] Data 0.006 (0.008) Batch 0.543 (0.545) Remain 01:16:49 MainLoss 0.7550 AuxLoss 0.7933 Loss 1.0723 Accuracy 0.6983.
[2025-02-11 05:28:39,737] INFO: Epoch: [1/5][540/1797] Data 0.006 (0.008) Batch 0.542 (0.545) Remain 01:16:43 MainLoss 1.3574 AuxLoss 1.3362 Loss 1.8919 Accuracy 0.5115.
[2025-02-11 05:28:45,168] INFO: Epoch: [1/5][550/1797] Data 0.006 (0.008) Batch 0.542 (0.545) Remain 01:16:38 MainLoss 1.3856 AuxLoss 1.1765 Loss 1.8562 Accuracy 0.4076.
[2025-02-11 05:28:50,591] INFO: Epoch: [1/5][560/1797] Data 0.005 (0.008) Batch 0.542 (0.545) Remain 01:16:32 MainLoss 1.3614 AuxLoss 1.4654 Loss 1.9476 Accuracy 0.5336.
[2025-02-11 05:28:56,016] INFO: Epoch: [1/5][570/1797] Data 0.006 (0.008) Batch 0.542 (0.545) Remain 01:16:26 MainLoss 0.7083 AuxLoss 0.8252 Loss 1.0384 Accuracy 0.7389.
[2025-02-11 05:29:01,441] INFO: Epoch: [1/5][580/1797] Data 0.006 (0.008) Batch 0.542 (0.545) Remain 01:16:20 MainLoss 1.1242 AuxLoss 1.0005 Loss 1.5243 Accuracy 0.6247.
[2025-02-11 05:29:06,866] INFO: Epoch: [1/5][590/1797] Data 0.006 (0.008) Batch 0.543 (0.545) Remain 01:16:14 MainLoss 0.7614 AuxLoss 0.9801 Loss 1.1534 Accuracy 0.7409.
[2025-02-11 05:29:12,288] INFO: Epoch: [1/5][600/1797] Data 0.005 (0.008) Batch 0.544 (0.545) Remain 01:16:09 MainLoss 0.4829 AuxLoss 0.6401 Loss 0.7390 Accuracy 0.8945.
[2025-02-11 05:29:17,712] INFO: Epoch: [1/5][610/1797] Data 0.005 (0.008) Batch 0.542 (0.545) Remain 01:16:03 MainLoss 0.7943 AuxLoss 1.0507 Loss 1.2146 Accuracy 0.6735.
[2025-02-11 05:29:23,136] INFO: Epoch: [1/5][620/1797] Data 0.006 (0.008) Batch 0.543 (0.545) Remain 01:15:57 MainLoss 0.4708 AuxLoss 0.7204 Loss 0.7590 Accuracy 0.7629.
[2025-02-11 05:29:28,782] INFO: Epoch: [1/5][630/1797] Data 0.006 (0.008) Batch 0.542 (0.545) Remain 01:15:54 MainLoss 0.5254 AuxLoss 0.5392 Loss 0.7411 Accuracy 0.8352.
[2025-02-11 05:29:34,205] INFO: Epoch: [1/5][640/1797] Data 0.006 (0.008) Batch 0.543 (0.545) Remain 01:15:48 MainLoss 2.2752 AuxLoss 2.4710 Loss 3.2636 Accuracy 0.2003.
[2025-02-11 05:29:39,631] INFO: Epoch: [1/5][650/1797] Data 0.006 (0.008) Batch 0.542 (0.545) Remain 01:15:43 MainLoss 1.3707 AuxLoss 1.1062 Loss 1.8132 Accuracy 0.5419.
[2025-02-11 05:29:45,053] INFO: Epoch: [1/5][660/1797] Data 0.006 (0.008) Batch 0.542 (0.545) Remain 01:15:37 MainLoss 0.8796 AuxLoss 1.1030 Loss 1.3208 Accuracy 0.6505.
[2025-02-11 05:29:50,469] INFO: Epoch: [1/5][670/1797] Data 0.006 (0.008) Batch 0.543 (0.545) Remain 01:15:31 MainLoss 0.8464 AuxLoss 0.9513 Loss 1.2270 Accuracy 0.7328.
[2025-02-11 05:29:55,889] INFO: Epoch: [1/5][680/1797] Data 0.006 (0.008) Batch 0.541 (0.545) Remain 01:15:25 MainLoss 0.5308 AuxLoss 0.5824 Loss 0.7637 Accuracy 0.8432.
[2025-02-11 05:30:01,312] INFO: Epoch: [1/5][690/1797] Data 0.006 (0.008) Batch 0.541 (0.545) Remain 01:15:19 MainLoss 1.2280 AuxLoss 1.4215 Loss 1.7965 Accuracy 0.4758.
[2025-02-11 05:30:06,729] INFO: Epoch: [1/5][700/1797] Data 0.005 (0.008) Batch 0.541 (0.545) Remain 01:15:13 MainLoss 0.4952 AuxLoss 0.6446 Loss 0.7531 Accuracy 0.8067.
[2025-02-11 05:30:12,150] INFO: Epoch: [1/5][710/1797] Data 0.006 (0.008) Batch 0.541 (0.545) Remain 01:15:08 MainLoss 1.1920 AuxLoss 1.3418 Loss 1.7287 Accuracy 0.5398.
[2025-02-11 05:30:17,569] INFO: Epoch: [1/5][720/1797] Data 0.005 (0.008) Batch 0.542 (0.545) Remain 01:15:02 MainLoss 0.3461 AuxLoss 0.7257 Loss 0.6364 Accuracy 0.9365.
[2025-02-11 05:30:22,988] INFO: Epoch: [1/5][730/1797] Data 0.005 (0.008) Batch 0.543 (0.545) Remain 01:14:56 MainLoss 0.4660 AuxLoss 0.5556 Loss 0.6882 Accuracy 0.8885.
[2025-02-11 05:30:28,405] INFO: Epoch: [1/5][740/1797] Data 0.006 (0.008) Batch 0.542 (0.545) Remain 01:14:50 MainLoss 0.7417 AuxLoss 0.7964 Loss 1.0603 Accuracy 0.7202.
[2025-02-11 05:30:33,822] INFO: Epoch: [1/5][750/1797] Data 0.005 (0.008) Batch 0.541 (0.545) Remain 01:14:45 MainLoss 0.8808 AuxLoss 0.9453 Loss 1.2590 Accuracy 0.6896.
[2025-02-11 05:30:39,243] INFO: Epoch: [1/5][760/1797] Data 0.005 (0.008) Batch 0.542 (0.545) Remain 01:14:39 MainLoss 0.4869 AuxLoss 0.7407 Loss 0.7832 Accuracy 0.8914.
[2025-02-11 05:30:44,663] INFO: Epoch: [1/5][770/1797] Data 0.006 (0.007) Batch 0.541 (0.545) Remain 01:14:33 MainLoss 1.1335 AuxLoss 1.4531 Loss 1.7147 Accuracy 0.6181.
[2025-02-11 05:30:50,078] INFO: Epoch: [1/5][780/1797] Data 0.005 (0.007) Batch 0.541 (0.545) Remain 01:14:27 MainLoss 1.3479 AuxLoss 1.3764 Loss 1.8984 Accuracy 0.5792.
[2025-02-11 05:30:55,499] INFO: Epoch: [1/5][790/1797] Data 0.006 (0.007) Batch 0.541 (0.544) Remain 01:14:22 MainLoss 1.0385 AuxLoss 0.9825 Loss 1.4315 Accuracy 0.5338.
[2025-02-11 05:31:00,929] INFO: Epoch: [1/5][800/1797] Data 0.005 (0.007) Batch 0.542 (0.544) Remain 01:14:16 MainLoss 0.7673 AuxLoss 0.9538 Loss 1.1489 Accuracy 0.7453.
[2025-02-11 05:31:06,353] INFO: Epoch: [1/5][810/1797] Data 0.005 (0.007) Batch 0.541 (0.544) Remain 01:14:10 MainLoss 1.2472 AuxLoss 1.3250 Loss 1.7771 Accuracy 0.4844.
[2025-02-11 05:31:11,777] INFO: Epoch: [1/5][820/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:14:05 MainLoss 1.3002 AuxLoss 1.5020 Loss 1.9010 Accuracy 0.5369.
[2025-02-11 05:31:17,200] INFO: Epoch: [1/5][830/1797] Data 0.005 (0.007) Batch 0.542 (0.544) Remain 01:13:59 MainLoss 0.5938 AuxLoss 0.6921 Loss 0.8706 Accuracy 0.7893.
[2025-02-11 05:31:22,630] INFO: Epoch: [1/5][840/1797] Data 0.005 (0.007) Batch 0.542 (0.544) Remain 01:13:54 MainLoss 0.6367 AuxLoss 0.6454 Loss 0.8948 Accuracy 0.7829.
[2025-02-11 05:31:28,047] INFO: Epoch: [1/5][850/1797] Data 0.005 (0.007) Batch 0.541 (0.544) Remain 01:13:48 MainLoss 0.7979 AuxLoss 0.8736 Loss 1.1473 Accuracy 0.7405.
[2025-02-11 05:31:33,463] INFO: Epoch: [1/5][860/1797] Data 0.005 (0.007) Batch 0.542 (0.544) Remain 01:13:42 MainLoss 0.6927 AuxLoss 1.0720 Loss 1.1215 Accuracy 0.6961.
[2025-02-11 05:31:38,885] INFO: Epoch: [1/5][870/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:13:36 MainLoss 0.6078 AuxLoss 0.8558 Loss 0.9501 Accuracy 0.8299.
[2025-02-11 05:31:44,310] INFO: Epoch: [1/5][880/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:13:31 MainLoss 0.5855 AuxLoss 0.5827 Loss 0.8185 Accuracy 0.7586.
[2025-02-11 05:31:49,736] INFO: Epoch: [1/5][890/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:13:25 MainLoss 0.7914 AuxLoss 0.9735 Loss 1.1808 Accuracy 0.7230.
[2025-02-11 05:31:55,167] INFO: Epoch: [1/5][900/1797] Data 0.005 (0.007) Batch 0.542 (0.544) Remain 01:13:20 MainLoss 0.7900 AuxLoss 0.7444 Loss 1.0878 Accuracy 0.7099.
[2025-02-11 05:32:00,594] INFO: Epoch: [1/5][910/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:13:14 MainLoss 0.9996 AuxLoss 0.9408 Loss 1.3759 Accuracy 0.5917.
[2025-02-11 05:32:06,020] INFO: Epoch: [1/5][920/1797] Data 0.005 (0.007) Batch 0.542 (0.544) Remain 01:13:09 MainLoss 1.0891 AuxLoss 0.9749 Loss 1.4791 Accuracy 0.4823.
[2025-02-11 05:32:11,448] INFO: Epoch: [1/5][930/1797] Data 0.005 (0.007) Batch 0.544 (0.544) Remain 01:13:03 MainLoss 1.0691 AuxLoss 1.2766 Loss 1.5797 Accuracy 0.4867.
[2025-02-11 05:32:16,874] INFO: Epoch: [1/5][940/1797] Data 0.005 (0.007) Batch 0.543 (0.544) Remain 01:12:57 MainLoss 0.7163 AuxLoss 1.1055 Loss 1.1585 Accuracy 0.7657.
[2025-02-11 05:32:22,303] INFO: Epoch: [1/5][950/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:12:52 MainLoss 1.1346 AuxLoss 1.2533 Loss 1.6359 Accuracy 0.5971.
[2025-02-11 05:32:27,736] INFO: Epoch: [1/5][960/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:12:46 MainLoss 0.9534 AuxLoss 0.9791 Loss 1.3450 Accuracy 0.5876.
[2025-02-11 05:32:33,165] INFO: Epoch: [1/5][970/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 01:12:41 MainLoss 0.3089 AuxLoss 0.5286 Loss 0.5203 Accuracy 0.9477.
[2025-02-11 05:32:38,595] INFO: Epoch: [1/5][980/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:12:35 MainLoss 1.1811 AuxLoss 1.2367 Loss 1.6758 Accuracy 0.5869.
[2025-02-11 05:32:44,023] INFO: Epoch: [1/5][990/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:12:30 MainLoss 1.2575 AuxLoss 1.1326 Loss 1.7106 Accuracy 0.5748.
[2025-02-11 05:32:49,454] INFO: Epoch: [1/5][1000/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:12:24 MainLoss 0.4601 AuxLoss 0.7186 Loss 0.7475 Accuracy 0.8865.
[2025-02-11 05:32:54,885] INFO: Epoch: [1/5][1010/1797] Data 0.006 (0.007) Batch 0.545 (0.544) Remain 01:12:19 MainLoss 0.8188 AuxLoss 0.9349 Loss 1.1928 Accuracy 0.7465.
[2025-02-11 05:33:00,313] INFO: Epoch: [1/5][1020/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:12:13 MainLoss 0.5242 AuxLoss 0.8580 Loss 0.8674 Accuracy 0.8350.
[2025-02-11 05:33:05,740] INFO: Epoch: [1/5][1030/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:12:08 MainLoss 0.3050 AuxLoss 0.5338 Loss 0.5185 Accuracy 0.9356.
[2025-02-11 05:33:11,167] INFO: Epoch: [1/5][1040/1797] Data 0.005 (0.007) Batch 0.543 (0.544) Remain 01:12:02 MainLoss 0.7407 AuxLoss 0.9585 Loss 1.1241 Accuracy 0.7197.
[2025-02-11 05:33:16,598] INFO: Epoch: [1/5][1050/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:11:57 MainLoss 1.0905 AuxLoss 0.8578 Loss 1.4337 Accuracy 0.5979.
[2025-02-11 05:33:22,034] INFO: Epoch: [1/5][1060/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:11:51 MainLoss 0.9874 AuxLoss 1.1945 Loss 1.4652 Accuracy 0.7270.
[2025-02-11 05:33:27,468] INFO: Epoch: [1/5][1070/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:11:46 MainLoss 0.7918 AuxLoss 0.9385 Loss 1.1672 Accuracy 0.7377.
[2025-02-11 05:33:32,897] INFO: Epoch: [1/5][1080/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:11:40 MainLoss 0.9369 AuxLoss 1.1152 Loss 1.3830 Accuracy 0.6809.
[2025-02-11 05:33:38,323] INFO: Epoch: [1/5][1090/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:11:35 MainLoss 1.2214 AuxLoss 1.2768 Loss 1.7321 Accuracy 0.4444.
[2025-02-11 05:33:43,751] INFO: Epoch: [1/5][1100/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:11:29 MainLoss 0.5873 AuxLoss 0.6225 Loss 0.8362 Accuracy 0.8546.
[2025-02-11 05:33:49,179] INFO: Epoch: [1/5][1110/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:11:23 MainLoss 1.1295 AuxLoss 1.1831 Loss 1.6027 Accuracy 0.5375.
[2025-02-11 05:33:54,601] INFO: Epoch: [1/5][1120/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:11:18 MainLoss 0.3044 AuxLoss 0.3954 Loss 0.4625 Accuracy 0.9010.
[2025-02-11 05:34:00,030] INFO: Epoch: [1/5][1130/1797] Data 0.006 (0.007) Batch 0.541 (0.544) Remain 01:11:12 MainLoss 0.6732 AuxLoss 0.8281 Loss 1.0044 Accuracy 0.7775.
[2025-02-11 05:34:05,452] INFO: Epoch: [1/5][1140/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:11:07 MainLoss 0.8942 AuxLoss 1.1341 Loss 1.3478 Accuracy 0.5924.
[2025-02-11 05:34:10,881] INFO: Epoch: [1/5][1150/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:11:01 MainLoss 0.5778 AuxLoss 0.9210 Loss 0.9463 Accuracy 0.7747.
[2025-02-11 05:34:16,309] INFO: Epoch: [1/5][1160/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:10:56 MainLoss 0.9995 AuxLoss 0.9808 Loss 1.3918 Accuracy 0.5843.
[2025-02-11 05:34:21,735] INFO: Epoch: [1/5][1170/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:10:50 MainLoss 0.7812 AuxLoss 1.0228 Loss 1.1903 Accuracy 0.7223.
[2025-02-11 05:34:27,162] INFO: Epoch: [1/5][1180/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:10:45 MainLoss 1.1088 AuxLoss 1.2805 Loss 1.6210 Accuracy 0.5201.
[2025-02-11 05:34:32,588] INFO: Epoch: [1/5][1190/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:10:39 MainLoss 0.8334 AuxLoss 1.0315 Loss 1.2460 Accuracy 0.6775.
[2025-02-11 05:34:38,010] INFO: Epoch: [1/5][1200/1797] Data 0.006 (0.007) Batch 0.541 (0.544) Remain 01:10:34 MainLoss 1.1246 AuxLoss 1.1836 Loss 1.5980 Accuracy 0.5392.
[2025-02-11 05:34:43,436] INFO: Epoch: [1/5][1210/1797] Data 0.006 (0.007) Batch 0.541 (0.544) Remain 01:10:28 MainLoss 0.9713 AuxLoss 1.1023 Loss 1.4122 Accuracy 0.6122.
[2025-02-11 05:34:48,863] INFO: Epoch: [1/5][1220/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:10:23 MainLoss 0.2291 AuxLoss 0.3873 Loss 0.3841 Accuracy 0.9526.
[2025-02-11 05:34:54,284] INFO: Epoch: [1/5][1230/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:10:17 MainLoss 0.9757 AuxLoss 0.9039 Loss 1.3373 Accuracy 0.6049.
[2025-02-11 05:34:59,707] INFO: Epoch: [1/5][1240/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:10:12 MainLoss 0.6693 AuxLoss 0.9807 Loss 1.0615 Accuracy 0.6804.
[2025-02-11 05:35:05,134] INFO: Epoch: [1/5][1250/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:10:06 MainLoss 0.3539 AuxLoss 0.7688 Loss 0.6614 Accuracy 0.8772.
[2025-02-11 05:35:10,561] INFO: Epoch: [1/5][1260/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:10:01 MainLoss 0.7351 AuxLoss 0.9342 Loss 1.1088 Accuracy 0.7363.
[2025-02-11 05:35:15,987] INFO: Epoch: [1/5][1270/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:09:55 MainLoss 0.7396 AuxLoss 0.8016 Loss 1.0603 Accuracy 0.7707.
[2025-02-11 05:35:21,415] INFO: Epoch: [1/5][1280/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:09:50 MainLoss 1.1745 AuxLoss 1.2073 Loss 1.6574 Accuracy 0.4871.
[2025-02-11 05:35:26,843] INFO: Epoch: [1/5][1290/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:09:44 MainLoss 1.0340 AuxLoss 1.0914 Loss 1.4705 Accuracy 0.5724.
[2025-02-11 05:35:32,277] INFO: Epoch: [1/5][1300/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:09:39 MainLoss 1.4484 AuxLoss 1.6851 Loss 2.1225 Accuracy 0.3486.
[2025-02-11 05:35:37,706] INFO: Epoch: [1/5][1310/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:09:33 MainLoss 0.4356 AuxLoss 0.6470 Loss 0.6944 Accuracy 0.8798.
[2025-02-11 05:35:43,139] INFO: Epoch: [1/5][1320/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:09:28 MainLoss 0.8629 AuxLoss 1.0270 Loss 1.2737 Accuracy 0.6027.
[2025-02-11 05:35:48,785] INFO: Epoch: [1/5][1330/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:09:23 MainLoss 0.4953 AuxLoss 0.6232 Loss 0.7446 Accuracy 0.8964.
[2025-02-11 05:35:54,218] INFO: Epoch: [1/5][1340/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:09:18 MainLoss 1.6105 AuxLoss 1.6978 Loss 2.2896 Accuracy 0.5286.
[2025-02-11 05:35:59,648] INFO: Epoch: [1/5][1350/1797] Data 0.006 (0.007) Batch 0.545 (0.544) Remain 01:09:12 MainLoss 0.8283 AuxLoss 1.2204 Loss 1.3164 Accuracy 0.7944.
[2025-02-11 05:36:05,076] INFO: Epoch: [1/5][1360/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 01:09:07 MainLoss 0.7393 AuxLoss 0.9167 Loss 1.1060 Accuracy 0.6157.
[2025-02-11 05:36:10,510] INFO: Epoch: [1/5][1370/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:09:01 MainLoss 0.8906 AuxLoss 0.8424 Loss 1.2275 Accuracy 0.6415.
[2025-02-11 05:36:15,944] INFO: Epoch: [1/5][1380/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:08:56 MainLoss 0.4870 AuxLoss 0.5153 Loss 0.6931 Accuracy 0.8674.
[2025-02-11 05:36:21,379] INFO: Epoch: [1/5][1390/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 01:08:50 MainLoss 0.5215 AuxLoss 0.5832 Loss 0.7548 Accuracy 0.9255.
[2025-02-11 05:36:26,814] INFO: Epoch: [1/5][1400/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:08:45 MainLoss 0.7230 AuxLoss 0.8160 Loss 1.0494 Accuracy 0.8232.
[2025-02-11 05:36:32,244] INFO: Epoch: [1/5][1410/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:08:40 MainLoss 0.6399 AuxLoss 0.6801 Loss 0.9120 Accuracy 0.8482.
[2025-02-11 05:36:37,680] INFO: Epoch: [1/5][1420/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:08:34 MainLoss 1.3141 AuxLoss 1.3245 Loss 1.8439 Accuracy 0.5891.
[2025-02-11 05:36:43,111] INFO: Epoch: [1/5][1430/1797] Data 0.005 (0.007) Batch 0.543 (0.544) Remain 01:08:29 MainLoss 0.7262 AuxLoss 0.8111 Loss 1.0506 Accuracy 0.7235.
[2025-02-11 05:36:48,545] INFO: Epoch: [1/5][1440/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 01:08:23 MainLoss 0.1847 AuxLoss 0.3386 Loss 0.3202 Accuracy 0.9776.
[2025-02-11 05:36:53,979] INFO: Epoch: [1/5][1450/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:08:18 MainLoss 0.7853 AuxLoss 0.8741 Loss 1.1350 Accuracy 0.6934.
[2025-02-11 05:36:59,410] INFO: Epoch: [1/5][1460/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:08:12 MainLoss 1.1778 AuxLoss 1.2496 Loss 1.6776 Accuracy 0.6057.
[2025-02-11 05:37:04,837] INFO: Epoch: [1/5][1470/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:08:07 MainLoss 0.9788 AuxLoss 0.9444 Loss 1.3566 Accuracy 0.6509.
[2025-02-11 05:37:10,264] INFO: Epoch: [1/5][1480/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:08:01 MainLoss 1.0915 AuxLoss 1.3478 Loss 1.6306 Accuracy 0.6420.
[2025-02-11 05:37:15,686] INFO: Epoch: [1/5][1490/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:07:56 MainLoss 0.6258 AuxLoss 0.6899 Loss 0.9018 Accuracy 0.7880.
[2025-02-11 05:37:21,111] INFO: Epoch: [1/5][1500/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:07:50 MainLoss 0.3695 AuxLoss 0.4370 Loss 0.5443 Accuracy 0.9031.
[2025-02-11 05:37:26,534] INFO: Epoch: [1/5][1510/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:07:45 MainLoss 0.9438 AuxLoss 0.9567 Loss 1.3265 Accuracy 0.6126.
[2025-02-11 05:37:31,959] INFO: Epoch: [1/5][1520/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:07:39 MainLoss 0.8232 AuxLoss 0.9470 Loss 1.2020 Accuracy 0.7998.
[2025-02-11 05:37:37,382] INFO: Epoch: [1/5][1530/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:07:34 MainLoss 0.5603 AuxLoss 0.9185 Loss 0.9277 Accuracy 0.7950.
[2025-02-11 05:37:42,808] INFO: Epoch: [1/5][1540/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:07:28 MainLoss 0.4131 AuxLoss 0.8170 Loss 0.7399 Accuracy 0.8197.
[2025-02-11 05:37:48,235] INFO: Epoch: [1/5][1550/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:07:23 MainLoss 0.5321 AuxLoss 0.6660 Loss 0.7986 Accuracy 0.8390.
[2025-02-11 05:37:53,663] INFO: Epoch: [1/5][1560/1797] Data 0.005 (0.007) Batch 0.542 (0.544) Remain 01:07:17 MainLoss 0.9245 AuxLoss 0.9976 Loss 1.3235 Accuracy 0.6468.
[2025-02-11 05:37:59,085] INFO: Epoch: [1/5][1570/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:07:12 MainLoss 0.9284 AuxLoss 1.0567 Loss 1.3511 Accuracy 0.6339.
[2025-02-11 05:38:04,510] INFO: Epoch: [1/5][1580/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:07:06 MainLoss 0.4169 AuxLoss 0.5430 Loss 0.6341 Accuracy 0.8859.
[2025-02-11 05:38:09,934] INFO: Epoch: [1/5][1590/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:07:01 MainLoss 0.8127 AuxLoss 1.0071 Loss 1.2156 Accuracy 0.5887.
[2025-02-11 05:38:15,389] INFO: Epoch: [1/5][1600/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 01:06:55 MainLoss 0.6026 AuxLoss 0.7435 Loss 0.9000 Accuracy 0.7872.
[2025-02-11 05:38:20,814] INFO: Epoch: [1/5][1610/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:06:50 MainLoss 0.9105 AuxLoss 1.0409 Loss 1.3268 Accuracy 0.6632.
[2025-02-11 05:38:26,239] INFO: Epoch: [1/5][1620/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:06:44 MainLoss 1.4657 AuxLoss 1.3222 Loss 1.9946 Accuracy 0.4253.
[2025-02-11 05:38:31,663] INFO: Epoch: [1/5][1630/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:06:39 MainLoss 0.6886 AuxLoss 0.8052 Loss 1.0106 Accuracy 0.7202.
[2025-02-11 05:38:37,090] INFO: Epoch: [1/5][1640/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:06:33 MainLoss 0.8457 AuxLoss 0.7633 Loss 1.1510 Accuracy 0.6444.
[2025-02-11 05:38:42,515] INFO: Epoch: [1/5][1650/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:06:28 MainLoss 0.6315 AuxLoss 0.6792 Loss 0.9032 Accuracy 0.8064.
[2025-02-11 05:38:47,940] INFO: Epoch: [1/5][1660/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:06:22 MainLoss 0.7073 AuxLoss 0.8250 Loss 1.0373 Accuracy 0.8348.
[2025-02-11 05:38:53,367] INFO: Epoch: [1/5][1670/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:06:17 MainLoss 0.7424 AuxLoss 1.0047 Loss 1.1443 Accuracy 0.8029.
[2025-02-11 05:38:58,791] INFO: Epoch: [1/5][1680/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:06:11 MainLoss 1.3217 AuxLoss 1.2658 Loss 1.8280 Accuracy 0.4183.
[2025-02-11 05:39:04,217] INFO: Epoch: [1/5][1690/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:06:06 MainLoss 0.4976 AuxLoss 0.9758 Loss 0.8879 Accuracy 0.8195.
[2025-02-11 05:39:09,650] INFO: Epoch: [1/5][1700/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 01:06:00 MainLoss 1.3060 AuxLoss 1.0343 Loss 1.7197 Accuracy 0.4925.
[2025-02-11 05:39:15,074] INFO: Epoch: [1/5][1710/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:05:55 MainLoss 0.4986 AuxLoss 0.4466 Loss 0.6773 Accuracy 0.7940.
[2025-02-11 05:39:20,492] INFO: Epoch: [1/5][1720/1797] Data 0.006 (0.007) Batch 0.541 (0.544) Remain 01:05:49 MainLoss 0.9681 AuxLoss 1.1604 Loss 1.4323 Accuracy 0.6703.
[2025-02-11 05:39:25,908] INFO: Epoch: [1/5][1730/1797] Data 0.006 (0.007) Batch 0.541 (0.544) Remain 01:05:44 MainLoss 1.2191 AuxLoss 0.9912 Loss 1.6156 Accuracy 0.5793.
[2025-02-11 05:39:31,332] INFO: Epoch: [1/5][1740/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 01:05:38 MainLoss 0.1951 AuxLoss 0.4142 Loss 0.3608 Accuracy 0.9610.
[2025-02-11 05:39:36,745] INFO: Epoch: [1/5][1750/1797] Data 0.005 (0.007) Batch 0.541 (0.544) Remain 01:05:33 MainLoss 0.9770 AuxLoss 1.1130 Loss 1.4222 Accuracy 0.6699.
[2025-02-11 05:39:42,161] INFO: Epoch: [1/5][1760/1797] Data 0.006 (0.007) Batch 0.541 (0.544) Remain 01:05:27 MainLoss 0.6909 AuxLoss 0.8258 Loss 1.0212 Accuracy 0.7211.
[2025-02-11 05:39:47,581] INFO: Epoch: [1/5][1770/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:05:22 MainLoss 0.4248 AuxLoss 0.5603 Loss 0.6489 Accuracy 0.8355.
[2025-02-11 05:39:53,003] INFO: Epoch: [1/5][1780/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:05:16 MainLoss 0.7068 AuxLoss 0.7841 Loss 1.0205 Accuracy 0.7746.
[2025-02-11 05:39:58,425] INFO: Epoch: [1/5][1790/1797] Data 0.006 (0.007) Batch 0.541 (0.544) Remain 01:05:11 MainLoss 0.4126 AuxLoss 0.8348 Loss 0.7465 Accuracy 0.8917.
[2025-02-11 05:40:02,251] INFO: Train result at epoch [1/5]: mIoU/mAcc/allAcc 0.3483/0.4556/0.6525.
[2025-02-11 05:40:02,253] INFO: Saving checkpoint to: exp/RescueNet/pspnet101/model/train_epoch_s1025_1.pth
[2025-02-11 05:40:08,652] INFO: Epoch: [2/5][10/1797] Data 0.005 (0.038) Batch 0.541 (0.574) Remain 01:08:41 MainLoss 0.7800 AuxLoss 0.8435 Loss 1.1174 Accuracy 0.7269.
[2025-02-11 05:40:14,073] INFO: Epoch: [2/5][20/1797] Data 0.005 (0.022) Batch 0.544 (0.558) Remain 01:06:40 MainLoss 0.7456 AuxLoss 0.8828 Loss 1.0987 Accuracy 0.7663.
[2025-02-11 05:40:19,498] INFO: Epoch: [2/5][30/1797] Data 0.006 (0.016) Batch 0.542 (0.553) Remain 01:05:57 MainLoss 0.8496 AuxLoss 0.8390 Loss 1.1852 Accuracy 0.7120.
[2025-02-11 05:40:24,928] INFO: Epoch: [2/5][40/1797] Data 0.006 (0.014) Batch 0.541 (0.550) Remain 01:05:34 MainLoss 1.4349 AuxLoss 1.3015 Loss 1.9555 Accuracy 0.4065.
[2025-02-11 05:40:30,349] INFO: Epoch: [2/5][50/1797] Data 0.005 (0.012) Batch 0.541 (0.549) Remain 01:05:16 MainLoss 0.6427 AuxLoss 0.7828 Loss 0.9558 Accuracy 0.6539.
[2025-02-11 05:40:35,767] INFO: Epoch: [2/5][60/1797] Data 0.005 (0.011) Batch 0.543 (0.548) Remain 01:05:03 MainLoss 0.2569 AuxLoss 0.5185 Loss 0.4643 Accuracy 0.9478.
[2025-02-11 05:40:41,189] INFO: Epoch: [2/5][70/1797] Data 0.006 (0.010) Batch 0.541 (0.547) Remain 01:04:52 MainLoss 0.7683 AuxLoss 0.9532 Loss 1.1495 Accuracy 0.7257.
[2025-02-11 05:40:46,616] INFO: Epoch: [2/5][80/1797] Data 0.006 (0.010) Batch 0.542 (0.546) Remain 01:04:43 MainLoss 0.9725 AuxLoss 1.2233 Loss 1.4618 Accuracy 0.6870.
[2025-02-11 05:40:52,037] INFO: Epoch: [2/5][90/1797] Data 0.006 (0.009) Batch 0.542 (0.546) Remain 01:04:34 MainLoss 1.0597 AuxLoss 1.0282 Loss 1.4709 Accuracy 0.6149.
[2025-02-11 05:40:57,463] INFO: Epoch: [2/5][100/1797] Data 0.005 (0.009) Batch 0.543 (0.546) Remain 01:04:26 MainLoss 0.4721 AuxLoss 0.5093 Loss 0.6758 Accuracy 0.8773.
[2025-02-11 05:41:02,885] INFO: Epoch: [2/5][110/1797] Data 0.006 (0.009) Batch 0.542 (0.545) Remain 01:04:19 MainLoss 0.6015 AuxLoss 0.7288 Loss 0.8930 Accuracy 0.8269.
[2025-02-11 05:41:08,307] INFO: Epoch: [2/5][120/1797] Data 0.006 (0.008) Batch 0.541 (0.545) Remain 01:04:11 MainLoss 0.6372 AuxLoss 0.5734 Loss 0.8666 Accuracy 0.6677.
[2025-02-11 05:41:13,729] INFO: Epoch: [2/5][130/1797] Data 0.006 (0.008) Batch 0.543 (0.545) Remain 01:04:04 MainLoss 0.5637 AuxLoss 0.7356 Loss 0.8580 Accuracy 0.8223.
[2025-02-11 05:41:19,154] INFO: Epoch: [2/5][140/1797] Data 0.006 (0.008) Batch 0.542 (0.545) Remain 01:03:58 MainLoss 1.1199 AuxLoss 1.1444 Loss 1.5777 Accuracy 0.5516.
[2025-02-11 05:41:24,577] INFO: Epoch: [2/5][150/1797] Data 0.006 (0.008) Batch 0.542 (0.544) Remain 01:03:51 MainLoss 0.6591 AuxLoss 0.7590 Loss 0.9627 Accuracy 0.7531.
[2025-02-11 05:41:29,995] INFO: Epoch: [2/5][160/1797] Data 0.005 (0.008) Batch 0.541 (0.544) Remain 01:03:45 MainLoss 0.5631 AuxLoss 0.5981 Loss 0.8024 Accuracy 0.8752.
[2025-02-11 05:41:35,417] INFO: Epoch: [2/5][170/1797] Data 0.006 (0.008) Batch 0.542 (0.544) Remain 01:03:38 MainLoss 1.0533 AuxLoss 1.1448 Loss 1.5112 Accuracy 0.5097.
[2025-02-11 05:41:40,838] INFO: Epoch: [2/5][180/1797] Data 0.006 (0.008) Batch 0.541 (0.544) Remain 01:03:32 MainLoss 0.9451 AuxLoss 1.5112 Loss 1.5496 Accuracy 0.5768.
[2025-02-11 05:41:46,257] INFO: Epoch: [2/5][190/1797] Data 0.005 (0.007) Batch 0.541 (0.544) Remain 01:03:26 MainLoss 1.0580 AuxLoss 1.2607 Loss 1.5623 Accuracy 0.5033.
[2025-02-11 05:41:51,672] INFO: Epoch: [2/5][200/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:03:20 MainLoss 0.4268 AuxLoss 0.4262 Loss 0.5973 Accuracy 0.8409.
[2025-02-11 05:41:57,089] INFO: Epoch: [2/5][210/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:03:13 MainLoss 1.4809 AuxLoss 1.4567 Loss 2.0636 Accuracy 0.2982.
[2025-02-11 05:42:02,511] INFO: Epoch: [2/5][220/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 01:03:08 MainLoss 0.5920 AuxLoss 0.8023 Loss 0.9129 Accuracy 0.7580.
[2025-02-11 05:42:07,933] INFO: Epoch: [2/5][230/1797] Data 0.006 (0.007) Batch 0.541 (0.544) Remain 01:03:02 MainLoss 0.6579 AuxLoss 0.7742 Loss 0.9676 Accuracy 0.7486.
[2025-02-11 05:42:13,349] INFO: Epoch: [2/5][240/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 01:02:56 MainLoss 0.4305 AuxLoss 0.5556 Loss 0.6527 Accuracy 0.8657.
[2025-02-11 05:42:18,779] INFO: Epoch: [2/5][250/1797] Data 0.006 (0.007) Batch 0.543 (0.543) Remain 01:02:50 MainLoss 1.0546 AuxLoss 1.1300 Loss 1.5066 Accuracy 0.5101.
[2025-02-11 05:42:24,210] INFO: Epoch: [2/5][260/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 01:02:45 MainLoss 0.7081 AuxLoss 0.7998 Loss 1.0280 Accuracy 0.7739.
[2025-02-11 05:42:29,645] INFO: Epoch: [2/5][270/1797] Data 0.007 (0.007) Batch 0.544 (0.543) Remain 01:02:39 MainLoss 1.0476 AuxLoss 1.1921 Loss 1.5244 Accuracy 0.6690.
[2025-02-11 05:42:35,079] INFO: Epoch: [2/5][280/1797] Data 0.007 (0.007) Batch 0.543 (0.543) Remain 01:02:34 MainLoss 0.7551 AuxLoss 1.0259 Loss 1.1654 Accuracy 0.7493.
[2025-02-11 05:42:40,510] INFO: Epoch: [2/5][290/1797] Data 0.007 (0.007) Batch 0.542 (0.543) Remain 01:02:28 MainLoss 1.1855 AuxLoss 1.1068 Loss 1.6283 Accuracy 0.3514.
[2025-02-11 05:42:45,945] INFO: Epoch: [2/5][300/1797] Data 0.007 (0.007) Batch 0.543 (0.543) Remain 01:02:23 MainLoss 0.8428 AuxLoss 0.8981 Loss 1.2020 Accuracy 0.5884.
[2025-02-11 05:42:51,374] INFO: Epoch: [2/5][310/1797] Data 0.007 (0.007) Batch 0.543 (0.543) Remain 01:02:17 MainLoss 0.6521 AuxLoss 0.8862 Loss 1.0066 Accuracy 0.8198.
[2025-02-11 05:42:56,802] INFO: Epoch: [2/5][320/1797] Data 0.006 (0.007) Batch 0.543 (0.543) Remain 01:02:12 MainLoss 0.9387 AuxLoss 1.0967 Loss 1.3773 Accuracy 0.6579.
[2025-02-11 05:43:02,236] INFO: Epoch: [2/5][330/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 01:02:06 MainLoss 0.7455 AuxLoss 0.9354 Loss 1.1197 Accuracy 0.7186.
[2025-02-11 05:43:07,660] INFO: Epoch: [2/5][340/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 01:02:01 MainLoss 0.8853 AuxLoss 1.2801 Loss 1.3974 Accuracy 0.7240.
[2025-02-11 05:43:13,090] INFO: Epoch: [2/5][350/1797] Data 0.006 (0.007) Batch 0.543 (0.543) Remain 01:01:55 MainLoss 0.9039 AuxLoss 0.9741 Loss 1.2935 Accuracy 0.6673.
[2025-02-11 05:43:18,521] INFO: Epoch: [2/5][360/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 01:01:50 MainLoss 2.0113 AuxLoss 1.6141 Loss 2.6570 Accuracy 0.1345.
[2025-02-11 05:43:23,948] INFO: Epoch: [2/5][370/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 01:01:44 MainLoss 0.7621 AuxLoss 0.7486 Loss 1.0615 Accuracy 0.7276.
[2025-02-11 05:43:29,377] INFO: Epoch: [2/5][380/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 01:01:38 MainLoss 0.7665 AuxLoss 0.8817 Loss 1.1191 Accuracy 0.6994.
[2025-02-11 05:43:34,806] INFO: Epoch: [2/5][390/1797] Data 0.006 (0.007) Batch 0.543 (0.543) Remain 01:01:33 MainLoss 0.8969 AuxLoss 1.2371 Loss 1.3918 Accuracy 0.6578.
[2025-02-11 05:43:40,236] INFO: Epoch: [2/5][400/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 01:01:28 MainLoss 0.7810 AuxLoss 0.8028 Loss 1.1021 Accuracy 0.5810.
[2025-02-11 05:43:45,664] INFO: Epoch: [2/5][410/1797] Data 0.006 (0.007) Batch 0.543 (0.543) Remain 01:01:22 MainLoss 0.6191 AuxLoss 0.4878 Loss 0.8143 Accuracy 0.6864.
[2025-02-11 05:43:51,093] INFO: Epoch: [2/5][420/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 01:01:16 MainLoss 0.8661 AuxLoss 0.7589 Loss 1.1697 Accuracy 0.6657.
[2025-02-11 05:43:56,521] INFO: Epoch: [2/5][430/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 01:01:11 MainLoss 0.6874 AuxLoss 0.6557 Loss 0.9497 Accuracy 0.7745.
[2025-02-11 05:44:01,945] INFO: Epoch: [2/5][440/1797] Data 0.006 (0.007) Batch 0.543 (0.543) Remain 01:01:05 MainLoss 0.4998 AuxLoss 0.6364 Loss 0.7544 Accuracy 0.8510.
[2025-02-11 05:44:07,369] INFO: Epoch: [2/5][450/1797] Data 0.006 (0.007) Batch 0.543 (0.543) Remain 01:01:00 MainLoss 0.9208 AuxLoss 0.9920 Loss 1.3176 Accuracy 0.6275.
[2025-02-11 05:44:12,792] INFO: Epoch: [2/5][460/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 01:00:54 MainLoss 0.6144 AuxLoss 0.7089 Loss 0.8979 Accuracy 0.7371.
[2025-02-11 05:44:18,222] INFO: Epoch: [2/5][470/1797] Data 0.006 (0.007) Batch 0.543 (0.543) Remain 01:00:49 MainLoss 0.6331 AuxLoss 0.9247 Loss 1.0030 Accuracy 0.7901.
[2025-02-11 05:44:23,651] INFO: Epoch: [2/5][480/1797] Data 0.006 (0.007) Batch 0.543 (0.543) Remain 01:00:43 MainLoss 1.0642 AuxLoss 1.1166 Loss 1.5108 Accuracy 0.5793.
[2025-02-11 05:44:29,075] INFO: Epoch: [2/5][490/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 01:00:38 MainLoss 0.7756 AuxLoss 0.7876 Loss 1.0907 Accuracy 0.6121.
[2025-02-11 05:44:34,495] INFO: Epoch: [2/5][500/1797] Data 0.006 (0.007) Batch 0.541 (0.543) Remain 01:00:32 MainLoss 1.4184 AuxLoss 1.1788 Loss 1.8899 Accuracy 0.3369.
[2025-02-11 05:44:39,923] INFO: Epoch: [2/5][510/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 01:00:27 MainLoss 0.4014 AuxLoss 0.6251 Loss 0.6514 Accuracy 0.8889.
[2025-02-11 05:44:45,350] INFO: Epoch: [2/5][520/1797] Data 0.006 (0.007) Batch 0.543 (0.543) Remain 01:00:21 MainLoss 0.5471 AuxLoss 0.5829 Loss 0.7802 Accuracy 0.7814.
[2025-02-11 05:44:50,774] INFO: Epoch: [2/5][530/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 01:00:16 MainLoss 0.6684 AuxLoss 0.7272 Loss 0.9592 Accuracy 0.7295.
[2025-02-11 05:44:56,203] INFO: Epoch: [2/5][540/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 01:00:10 MainLoss 0.6147 AuxLoss 0.8861 Loss 0.9692 Accuracy 0.8146.
[2025-02-11 05:45:01,633] INFO: Epoch: [2/5][550/1797] Data 0.006 (0.007) Batch 0.543 (0.543) Remain 01:00:05 MainLoss 0.7467 AuxLoss 0.8919 Loss 1.1035 Accuracy 0.7502.
[2025-02-11 05:45:07,060] INFO: Epoch: [2/5][560/1797] Data 0.006 (0.007) Batch 0.543 (0.543) Remain 00:59:59 MainLoss 0.5644 AuxLoss 0.6072 Loss 0.8072 Accuracy 0.8167.
[2025-02-11 05:45:12,489] INFO: Epoch: [2/5][570/1797] Data 0.006 (0.007) Batch 0.543 (0.543) Remain 00:59:54 MainLoss 0.7047 AuxLoss 0.6365 Loss 0.9593 Accuracy 0.8045.
[2025-02-11 05:45:17,917] INFO: Epoch: [2/5][580/1797] Data 0.006 (0.007) Batch 0.544 (0.543) Remain 00:59:48 MainLoss 0.5123 AuxLoss 0.6414 Loss 0.7689 Accuracy 0.8238.
[2025-02-11 05:45:23,339] INFO: Epoch: [2/5][590/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 00:59:43 MainLoss 1.0272 AuxLoss 0.9958 Loss 1.4255 Accuracy 0.6299.
[2025-02-11 05:45:28,763] INFO: Epoch: [2/5][600/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 00:59:37 MainLoss 0.1715 AuxLoss 0.3483 Loss 0.3109 Accuracy 0.9765.
[2025-02-11 05:45:34,185] INFO: Epoch: [2/5][610/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 00:59:32 MainLoss 0.5035 AuxLoss 0.7988 Loss 0.8230 Accuracy 0.8157.
[2025-02-11 05:45:39,607] INFO: Epoch: [2/5][620/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 00:59:26 MainLoss 0.4500 AuxLoss 0.4738 Loss 0.6395 Accuracy 0.8702.
[2025-02-11 05:45:45,034] INFO: Epoch: [2/5][630/1797] Data 0.006 (0.007) Batch 0.542 (0.543) Remain 00:59:21 MainLoss 0.6597 AuxLoss 0.9112 Loss 1.0242 Accuracy 0.7979.
[2025-02-11 05:45:50,458] INFO: Epoch: [2/5][640/1797] Data 0.006 (0.007) Batch 0.541 (0.543) Remain 00:59:15 MainLoss 0.9684 AuxLoss 0.9396 Loss 1.3443 Accuracy 0.6243.
[2025-02-11 05:45:55,874] INFO: Epoch: [2/5][650/1797] Data 0.006 (0.006) Batch 0.541 (0.543) Remain 00:59:10 MainLoss 1.2452 AuxLoss 1.0653 Loss 1.6714 Accuracy 0.5391.
[2025-02-11 05:46:01,298] INFO: Epoch: [2/5][660/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:59:04 MainLoss 0.3827 AuxLoss 0.5599 Loss 0.6066 Accuracy 0.8809.
[2025-02-11 05:46:06,727] INFO: Epoch: [2/5][670/1797] Data 0.006 (0.006) Batch 0.541 (0.543) Remain 00:58:59 MainLoss 0.7927 AuxLoss 0.9812 Loss 1.1852 Accuracy 0.6878.
[2025-02-11 05:46:12,144] INFO: Epoch: [2/5][680/1797] Data 0.006 (0.006) Batch 0.541 (0.543) Remain 00:58:53 MainLoss 0.8955 AuxLoss 0.9562 Loss 1.2780 Accuracy 0.6085.
[2025-02-11 05:46:17,560] INFO: Epoch: [2/5][690/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:58:48 MainLoss 0.4264 AuxLoss 0.7183 Loss 0.7138 Accuracy 0.8556.
[2025-02-11 05:46:22,979] INFO: Epoch: [2/5][700/1797] Data 0.006 (0.006) Batch 0.541 (0.543) Remain 00:58:42 MainLoss 1.4824 AuxLoss 1.4538 Loss 2.0640 Accuracy 0.4120.
[2025-02-11 05:46:28,405] INFO: Epoch: [2/5][710/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:58:37 MainLoss 0.4548 AuxLoss 0.6941 Loss 0.7324 Accuracy 0.8636.
[2025-02-11 05:46:33,827] INFO: Epoch: [2/5][720/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:58:31 MainLoss 0.3606 AuxLoss 0.4476 Loss 0.5397 Accuracy 0.9077.
[2025-02-11 05:46:39,248] INFO: Epoch: [2/5][730/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:58:26 MainLoss 0.7313 AuxLoss 1.3508 Loss 1.2716 Accuracy 0.6733.
[2025-02-11 05:46:44,677] INFO: Epoch: [2/5][740/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:58:20 MainLoss 0.5851 AuxLoss 0.7848 Loss 0.8990 Accuracy 0.8449.
[2025-02-11 05:46:50,106] INFO: Epoch: [2/5][750/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:58:15 MainLoss 0.3782 AuxLoss 0.5049 Loss 0.5802 Accuracy 0.9099.
[2025-02-11 05:46:55,533] INFO: Epoch: [2/5][760/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:58:09 MainLoss 0.6014 AuxLoss 0.6495 Loss 0.8612 Accuracy 0.7853.
[2025-02-11 05:47:00,958] INFO: Epoch: [2/5][770/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:58:04 MainLoss 0.5580 AuxLoss 0.5970 Loss 0.7968 Accuracy 0.7944.
[2025-02-11 05:47:06,386] INFO: Epoch: [2/5][780/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:57:59 MainLoss 0.5389 AuxLoss 0.6460 Loss 0.7974 Accuracy 0.8176.
[2025-02-11 05:47:11,804] INFO: Epoch: [2/5][790/1797] Data 0.006 (0.006) Batch 0.541 (0.543) Remain 00:57:53 MainLoss 1.2115 AuxLoss 0.9886 Loss 1.6069 Accuracy 0.4425.
[2025-02-11 05:47:17,235] INFO: Epoch: [2/5][800/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:57:48 MainLoss 0.6716 AuxLoss 0.8278 Loss 1.0027 Accuracy 0.7703.
[2025-02-11 05:47:22,654] INFO: Epoch: [2/5][810/1797] Data 0.006 (0.006) Batch 0.541 (0.543) Remain 00:57:42 MainLoss 0.2112 AuxLoss 0.3522 Loss 0.3521 Accuracy 0.9558.
[2025-02-11 05:47:28,075] INFO: Epoch: [2/5][820/1797] Data 0.006 (0.006) Batch 0.541 (0.543) Remain 00:57:37 MainLoss 0.7954 AuxLoss 0.6209 Loss 1.0438 Accuracy 0.6681.
[2025-02-11 05:47:33,494] INFO: Epoch: [2/5][830/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:57:31 MainLoss 0.4344 AuxLoss 0.4518 Loss 0.6151 Accuracy 0.8811.
[2025-02-11 05:47:38,910] INFO: Epoch: [2/5][840/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:57:26 MainLoss 0.2482 AuxLoss 0.3793 Loss 0.3999 Accuracy 0.9424.
[2025-02-11 05:47:44,333] INFO: Epoch: [2/5][850/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:57:20 MainLoss 0.5267 AuxLoss 0.6209 Loss 0.7751 Accuracy 0.8271.
[2025-02-11 05:47:49,755] INFO: Epoch: [2/5][860/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:57:15 MainLoss 0.7293 AuxLoss 0.8387 Loss 1.0647 Accuracy 0.7004.
[2025-02-11 05:47:55,177] INFO: Epoch: [2/5][870/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:57:09 MainLoss 0.7304 AuxLoss 0.7697 Loss 1.0383 Accuracy 0.7047.
[2025-02-11 05:48:00,596] INFO: Epoch: [2/5][880/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:57:04 MainLoss 0.5216 AuxLoss 0.7377 Loss 0.8167 Accuracy 0.8665.
[2025-02-11 05:48:06,014] INFO: Epoch: [2/5][890/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:56:58 MainLoss 0.5790 AuxLoss 0.9513 Loss 0.9596 Accuracy 0.8290.
[2025-02-11 05:48:11,435] INFO: Epoch: [2/5][900/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:56:53 MainLoss 0.3683 AuxLoss 0.5702 Loss 0.5964 Accuracy 0.8837.
[2025-02-11 05:48:16,866] INFO: Epoch: [2/5][910/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:56:47 MainLoss 0.3396 AuxLoss 0.4112 Loss 0.5041 Accuracy 0.8699.
[2025-02-11 05:48:22,293] INFO: Epoch: [2/5][920/1797] Data 0.006 (0.006) Batch 0.541 (0.543) Remain 00:56:42 MainLoss 0.9393 AuxLoss 0.7857 Loss 1.2536 Accuracy 0.5825.
[2025-02-11 05:48:27,716] INFO: Epoch: [2/5][930/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:56:36 MainLoss 0.8358 AuxLoss 0.9256 Loss 1.2060 Accuracy 0.6716.
[2025-02-11 05:48:33,143] INFO: Epoch: [2/5][940/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:56:31 MainLoss 0.6849 AuxLoss 0.7166 Loss 0.9715 Accuracy 0.7904.
[2025-02-11 05:48:38,563] INFO: Epoch: [2/5][950/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:56:25 MainLoss 0.3647 AuxLoss 0.5745 Loss 0.5945 Accuracy 0.8904.
[2025-02-11 05:48:43,986] INFO: Epoch: [2/5][960/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:56:20 MainLoss 0.5827 AuxLoss 0.6587 Loss 0.8462 Accuracy 0.7899.
[2025-02-11 05:48:49,405] INFO: Epoch: [2/5][970/1797] Data 0.005 (0.006) Batch 0.542 (0.543) Remain 00:56:14 MainLoss 0.4644 AuxLoss 0.5391 Loss 0.6800 Accuracy 0.8750.
[2025-02-11 05:48:54,825] INFO: Epoch: [2/5][980/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:56:09 MainLoss 1.0601 AuxLoss 1.1667 Loss 1.5268 Accuracy 0.5219.
[2025-02-11 05:49:00,247] INFO: Epoch: [2/5][990/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:56:04 MainLoss 0.6055 AuxLoss 0.7020 Loss 0.8863 Accuracy 0.8278.
[2025-02-11 05:49:05,671] INFO: Epoch: [2/5][1000/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:55:58 MainLoss 0.9997 AuxLoss 1.0508 Loss 1.4200 Accuracy 0.5827.
[2025-02-11 05:49:11,098] INFO: Epoch: [2/5][1010/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:55:53 MainLoss 0.2514 AuxLoss 0.6055 Loss 0.4936 Accuracy 0.9429.
[2025-02-11 05:49:16,526] INFO: Epoch: [2/5][1020/1797] Data 0.006 (0.006) Batch 0.541 (0.543) Remain 00:55:47 MainLoss 0.8160 AuxLoss 0.8188 Loss 1.1435 Accuracy 0.6922.
[2025-02-11 05:49:21,949] INFO: Epoch: [2/5][1030/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:55:42 MainLoss 0.5126 AuxLoss 0.6434 Loss 0.7700 Accuracy 0.8177.
[2025-02-11 05:49:27,374] INFO: Epoch: [2/5][1040/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:55:36 MainLoss 0.3709 AuxLoss 0.4425 Loss 0.5479 Accuracy 0.8778.
[2025-02-11 05:49:32,799] INFO: Epoch: [2/5][1050/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:55:31 MainLoss 0.3443 AuxLoss 0.4230 Loss 0.5135 Accuracy 0.9109.
[2025-02-11 05:49:38,233] INFO: Epoch: [2/5][1060/1797] Data 0.006 (0.006) Batch 0.545 (0.543) Remain 00:55:26 MainLoss 0.7218 AuxLoss 0.5334 Loss 0.9352 Accuracy 0.6923.
[2025-02-11 05:49:43,657] INFO: Epoch: [2/5][1070/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:55:20 MainLoss 0.8484 AuxLoss 0.8893 Loss 1.2041 Accuracy 0.6181.
[2025-02-11 05:49:49,083] INFO: Epoch: [2/5][1080/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:55:15 MainLoss 0.5221 AuxLoss 0.5914 Loss 0.7587 Accuracy 0.8613.
[2025-02-11 05:49:54,511] INFO: Epoch: [2/5][1090/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:55:09 MainLoss 0.6848 AuxLoss 0.8154 Loss 1.0109 Accuracy 0.7527.
[2025-02-11 05:49:59,940] INFO: Epoch: [2/5][1100/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:55:04 MainLoss 0.5800 AuxLoss 0.6333 Loss 0.8333 Accuracy 0.7436.
[2025-02-11 05:50:05,362] INFO: Epoch: [2/5][1110/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:54:58 MainLoss 0.6495 AuxLoss 0.7713 Loss 0.9580 Accuracy 0.8395.
[2025-02-11 05:50:10,791] INFO: Epoch: [2/5][1120/1797] Data 0.006 (0.006) Batch 0.545 (0.543) Remain 00:54:53 MainLoss 1.0499 AuxLoss 1.1083 Loss 1.4932 Accuracy 0.5877.
[2025-02-11 05:50:16,216] INFO: Epoch: [2/5][1130/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:54:47 MainLoss 0.7597 AuxLoss 1.0177 Loss 1.1668 Accuracy 0.7851.
[2025-02-11 05:50:21,644] INFO: Epoch: [2/5][1140/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:54:42 MainLoss 0.8091 AuxLoss 0.9604 Loss 1.1933 Accuracy 0.7966.
[2025-02-11 05:50:27,069] INFO: Epoch: [2/5][1150/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:54:37 MainLoss 1.5093 AuxLoss 1.3746 Loss 2.0592 Accuracy 0.4874.
[2025-02-11 05:50:32,497] INFO: Epoch: [2/5][1160/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:54:31 MainLoss 0.4588 AuxLoss 0.6758 Loss 0.7291 Accuracy 0.8328.
[2025-02-11 05:50:37,934] INFO: Epoch: [2/5][1170/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:54:26 MainLoss 0.6158 AuxLoss 0.7856 Loss 0.9300 Accuracy 0.7345.
[2025-02-11 05:50:43,361] INFO: Epoch: [2/5][1180/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:54:20 MainLoss 0.8253 AuxLoss 0.8933 Loss 1.1826 Accuracy 0.7458.
[2025-02-11 05:50:48,783] INFO: Epoch: [2/5][1190/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:54:15 MainLoss 0.3339 AuxLoss 0.3964 Loss 0.4925 Accuracy 0.9385.
[2025-02-11 05:50:54,212] INFO: Epoch: [2/5][1200/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:54:09 MainLoss 0.3924 AuxLoss 0.4973 Loss 0.5913 Accuracy 0.8162.
[2025-02-11 05:50:59,641] INFO: Epoch: [2/5][1210/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:54:04 MainLoss 0.4386 AuxLoss 0.5957 Loss 0.6768 Accuracy 0.8825.
[2025-02-11 05:51:05,069] INFO: Epoch: [2/5][1220/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:53:59 MainLoss 1.2031 AuxLoss 1.1055 Loss 1.6453 Accuracy 0.5388.
[2025-02-11 05:51:10,495] INFO: Epoch: [2/5][1230/1797] Data 0.005 (0.006) Batch 0.542 (0.543) Remain 00:53:53 MainLoss 0.2808 AuxLoss 0.3942 Loss 0.4385 Accuracy 0.9374.
[2025-02-11 05:51:15,929] INFO: Epoch: [2/5][1240/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:53:48 MainLoss 0.4654 AuxLoss 0.5573 Loss 0.6883 Accuracy 0.8508.
[2025-02-11 05:51:21,351] INFO: Epoch: [2/5][1250/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:53:42 MainLoss 0.5404 AuxLoss 0.6802 Loss 0.8125 Accuracy 0.8146.
[2025-02-11 05:51:26,774] INFO: Epoch: [2/5][1260/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:53:37 MainLoss 0.5278 AuxLoss 0.5347 Loss 0.7417 Accuracy 0.7690.
[2025-02-11 05:51:32,195] INFO: Epoch: [2/5][1270/1797] Data 0.005 (0.006) Batch 0.541 (0.543) Remain 00:53:31 MainLoss 0.8228 AuxLoss 0.7977 Loss 1.1419 Accuracy 0.7438.
[2025-02-11 05:51:37,619] INFO: Epoch: [2/5][1280/1797] Data 0.006 (0.006) Batch 0.544 (0.543) Remain 00:53:26 MainLoss 0.6341 AuxLoss 0.9406 Loss 1.0103 Accuracy 0.7437.
[2025-02-11 05:51:43,050] INFO: Epoch: [2/5][1290/1797] Data 0.006 (0.006) Batch 0.541 (0.543) Remain 00:53:21 MainLoss 0.6832 AuxLoss 0.5215 Loss 0.8918 Accuracy 0.6831.
[2025-02-11 05:51:48,473] INFO: Epoch: [2/5][1300/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:53:15 MainLoss 0.7466 AuxLoss 0.8411 Loss 1.0831 Accuracy 0.6923.
[2025-02-11 05:51:53,921] INFO: Epoch: [2/5][1310/1797] Data 0.007 (0.006) Batch 0.546 (0.543) Remain 00:53:10 MainLoss 0.5571 AuxLoss 0.5433 Loss 0.7744 Accuracy 0.8388.
[2025-02-11 05:51:59,365] INFO: Epoch: [2/5][1320/1797] Data 0.007 (0.006) Batch 0.543 (0.543) Remain 00:53:04 MainLoss 1.4120 AuxLoss 1.4678 Loss 1.9991 Accuracy 0.3536.
[2025-02-11 05:52:04,803] INFO: Epoch: [2/5][1330/1797] Data 0.007 (0.006) Batch 0.545 (0.543) Remain 00:52:59 MainLoss 0.8158 AuxLoss 1.0525 Loss 1.2368 Accuracy 0.6734.
[2025-02-11 05:52:10,255] INFO: Epoch: [2/5][1340/1797] Data 0.007 (0.006) Batch 0.543 (0.543) Remain 00:52:54 MainLoss 0.6592 AuxLoss 0.7753 Loss 0.9693 Accuracy 0.7098.
[2025-02-11 05:52:15,700] INFO: Epoch: [2/5][1350/1797] Data 0.007 (0.006) Batch 0.544 (0.543) Remain 00:52:48 MainLoss 0.7513 AuxLoss 1.0192 Loss 1.1590 Accuracy 0.7405.
[2025-02-11 05:52:21,160] INFO: Epoch: [2/5][1360/1797] Data 0.007 (0.006) Batch 0.544 (0.543) Remain 00:52:43 MainLoss 0.3755 AuxLoss 0.6229 Loss 0.6246 Accuracy 0.8957.
[2025-02-11 05:52:26,608] INFO: Epoch: [2/5][1370/1797] Data 0.007 (0.006) Batch 0.544 (0.543) Remain 00:52:38 MainLoss 0.7080 AuxLoss 0.8768 Loss 1.0587 Accuracy 0.7080.
[2025-02-11 05:52:32,057] INFO: Epoch: [2/5][1380/1797] Data 0.007 (0.006) Batch 0.546 (0.543) Remain 00:52:32 MainLoss 0.8909 AuxLoss 1.0258 Loss 1.3012 Accuracy 0.6907.
[2025-02-11 05:52:37,504] INFO: Epoch: [2/5][1390/1797] Data 0.007 (0.006) Batch 0.544 (0.543) Remain 00:52:27 MainLoss 1.0228 AuxLoss 0.9753 Loss 1.4129 Accuracy 0.5921.
[2025-02-11 05:52:42,954] INFO: Epoch: [2/5][1400/1797] Data 0.007 (0.006) Batch 0.544 (0.543) Remain 00:52:22 MainLoss 0.5341 AuxLoss 0.5391 Loss 0.7497 Accuracy 0.7380.
[2025-02-11 05:52:48,395] INFO: Epoch: [2/5][1410/1797] Data 0.007 (0.006) Batch 0.542 (0.543) Remain 00:52:16 MainLoss 1.3355 AuxLoss 1.3399 Loss 1.8714 Accuracy 0.5045.
[2025-02-11 05:52:53,837] INFO: Epoch: [2/5][1420/1797] Data 0.007 (0.006) Batch 0.544 (0.543) Remain 00:52:11 MainLoss 0.9219 AuxLoss 1.1265 Loss 1.3725 Accuracy 0.6556.
[2025-02-11 05:52:59,285] INFO: Epoch: [2/5][1430/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:52:06 MainLoss 0.6790 AuxLoss 0.6707 Loss 0.9473 Accuracy 0.7374.
[2025-02-11 05:53:04,731] INFO: Epoch: [2/5][1440/1797] Data 0.007 (0.006) Batch 0.545 (0.543) Remain 00:52:00 MainLoss 0.2991 AuxLoss 0.5318 Loss 0.5118 Accuracy 0.9494.
[2025-02-11 05:53:10,177] INFO: Epoch: [2/5][1450/1797] Data 0.007 (0.006) Batch 0.544 (0.543) Remain 00:51:55 MainLoss 0.9946 AuxLoss 1.2349 Loss 1.4886 Accuracy 0.5044.
[2025-02-11 05:53:15,628] INFO: Epoch: [2/5][1460/1797] Data 0.007 (0.006) Batch 0.545 (0.543) Remain 00:51:50 MainLoss 0.4524 AuxLoss 0.6325 Loss 0.7054 Accuracy 0.8551.
[2025-02-11 05:53:21,072] INFO: Epoch: [2/5][1470/1797] Data 0.007 (0.006) Batch 0.545 (0.543) Remain 00:51:44 MainLoss 0.8873 AuxLoss 0.8602 Loss 1.2314 Accuracy 0.7460.
[2025-02-11 05:53:26,513] INFO: Epoch: [2/5][1480/1797] Data 0.007 (0.006) Batch 0.543 (0.543) Remain 00:51:39 MainLoss 1.3185 AuxLoss 1.1736 Loss 1.7879 Accuracy 0.4321.
[2025-02-11 05:53:31,962] INFO: Epoch: [2/5][1490/1797] Data 0.007 (0.006) Batch 0.546 (0.543) Remain 00:51:33 MainLoss 1.2836 AuxLoss 1.6773 Loss 1.9546 Accuracy 0.4872.
[2025-02-11 05:53:37,406] INFO: Epoch: [2/5][1500/1797] Data 0.007 (0.006) Batch 0.545 (0.543) Remain 00:51:28 MainLoss 0.4364 AuxLoss 0.4506 Loss 0.6167 Accuracy 0.8337.
[2025-02-11 05:53:42,854] INFO: Epoch: [2/5][1510/1797] Data 0.007 (0.006) Batch 0.546 (0.543) Remain 00:51:23 MainLoss 0.1071 AuxLoss 0.2620 Loss 0.2119 Accuracy 0.9885.
[2025-02-11 05:53:48,297] INFO: Epoch: [2/5][1520/1797] Data 0.007 (0.006) Batch 0.544 (0.543) Remain 00:51:17 MainLoss 0.5512 AuxLoss 0.6335 Loss 0.8046 Accuracy 0.8116.
[2025-02-11 05:53:53,748] INFO: Epoch: [2/5][1530/1797] Data 0.006 (0.006) Batch 0.545 (0.543) Remain 00:51:12 MainLoss 0.5652 AuxLoss 0.6307 Loss 0.8175 Accuracy 0.7928.
[2025-02-11 05:53:59,194] INFO: Epoch: [2/5][1540/1797] Data 0.007 (0.006) Batch 0.544 (0.543) Remain 00:51:07 MainLoss 1.0678 AuxLoss 1.2583 Loss 1.5711 Accuracy 0.4763.
[2025-02-11 05:54:04,639] INFO: Epoch: [2/5][1550/1797] Data 0.007 (0.006) Batch 0.546 (0.543) Remain 00:51:01 MainLoss 0.3845 AuxLoss 0.4600 Loss 0.5685 Accuracy 0.9194.
[2025-02-11 05:54:10,084] INFO: Epoch: [2/5][1560/1797] Data 0.007 (0.006) Batch 0.543 (0.543) Remain 00:50:56 MainLoss 0.6707 AuxLoss 1.1011 Loss 1.1111 Accuracy 0.7555.
[2025-02-11 05:54:15,526] INFO: Epoch: [2/5][1570/1797] Data 0.007 (0.006) Batch 0.546 (0.543) Remain 00:50:50 MainLoss 0.4099 AuxLoss 0.5730 Loss 0.6391 Accuracy 0.8715.
[2025-02-11 05:54:20,967] INFO: Epoch: [2/5][1580/1797] Data 0.007 (0.006) Batch 0.545 (0.543) Remain 00:50:45 MainLoss 0.3312 AuxLoss 0.3071 Loss 0.4540 Accuracy 0.8833.
[2025-02-11 05:54:26,406] INFO: Epoch: [2/5][1590/1797] Data 0.006 (0.006) Batch 0.544 (0.543) Remain 00:50:40 MainLoss 0.8962 AuxLoss 1.1336 Loss 1.3496 Accuracy 0.6311.
[2025-02-11 05:54:31,844] INFO: Epoch: [2/5][1600/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:50:34 MainLoss 0.3504 AuxLoss 0.6166 Loss 0.5971 Accuracy 0.9169.
[2025-02-11 05:54:37,279] INFO: Epoch: [2/5][1610/1797] Data 0.007 (0.006) Batch 0.544 (0.543) Remain 00:50:29 MainLoss 0.8433 AuxLoss 1.0546 Loss 1.2652 Accuracy 0.7112.
[2025-02-11 05:54:42,720] INFO: Epoch: [2/5][1620/1797] Data 0.006 (0.006) Batch 0.546 (0.543) Remain 00:50:23 MainLoss 0.6194 AuxLoss 0.5177 Loss 0.8264 Accuracy 0.7049.
[2025-02-11 05:54:48,165] INFO: Epoch: [2/5][1630/1797] Data 0.006 (0.006) Batch 0.545 (0.543) Remain 00:50:18 MainLoss 0.4309 AuxLoss 0.6337 Loss 0.6843 Accuracy 0.8953.
[2025-02-11 05:54:53,603] INFO: Epoch: [2/5][1640/1797] Data 0.006 (0.006) Batch 0.545 (0.543) Remain 00:50:13 MainLoss 0.7141 AuxLoss 0.6801 Loss 0.9862 Accuracy 0.8017.
[2025-02-11 05:54:59,049] INFO: Epoch: [2/5][1650/1797] Data 0.007 (0.006) Batch 0.545 (0.543) Remain 00:50:07 MainLoss 0.8783 AuxLoss 0.9924 Loss 1.2753 Accuracy 0.7467.
[2025-02-11 05:55:04,495] INFO: Epoch: [2/5][1660/1797] Data 0.007 (0.006) Batch 0.544 (0.543) Remain 00:50:02 MainLoss 0.4606 AuxLoss 0.6546 Loss 0.7225 Accuracy 0.8850.
[2025-02-11 05:55:09,935] INFO: Epoch: [2/5][1670/1797] Data 0.006 (0.006) Batch 0.545 (0.543) Remain 00:49:56 MainLoss 0.4023 AuxLoss 0.4268 Loss 0.5730 Accuracy 0.8430.
[2025-02-11 05:55:15,370] INFO: Epoch: [2/5][1680/1797] Data 0.006 (0.006) Batch 0.545 (0.543) Remain 00:49:51 MainLoss 0.5687 AuxLoss 0.5566 Loss 0.7913 Accuracy 0.7713.
[2025-02-11 05:55:20,809] INFO: Epoch: [2/5][1690/1797] Data 0.006 (0.006) Batch 0.544 (0.543) Remain 00:49:46 MainLoss 0.4239 AuxLoss 0.4756 Loss 0.6142 Accuracy 0.8551.
[2025-02-11 05:55:26,247] INFO: Epoch: [2/5][1700/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:49:40 MainLoss 0.5451 AuxLoss 0.5442 Loss 0.7628 Accuracy 0.8480.
[2025-02-11 05:55:31,684] INFO: Epoch: [2/5][1710/1797] Data 0.006 (0.006) Batch 0.544 (0.543) Remain 00:49:35 MainLoss 0.3081 AuxLoss 0.3741 Loss 0.4577 Accuracy 0.9168.
[2025-02-11 05:55:37,123] INFO: Epoch: [2/5][1720/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:49:29 MainLoss 0.8666 AuxLoss 1.1667 Loss 1.3333 Accuracy 0.6457.
[2025-02-11 05:55:42,563] INFO: Epoch: [2/5][1730/1797] Data 0.007 (0.006) Batch 0.544 (0.543) Remain 00:49:24 MainLoss 0.3686 AuxLoss 0.4680 Loss 0.5558 Accuracy 0.8970.
[2025-02-11 05:55:47,999] INFO: Epoch: [2/5][1740/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:49:19 MainLoss 0.5041 AuxLoss 0.6724 Loss 0.7731 Accuracy 0.8129.
[2025-02-11 05:55:53,445] INFO: Epoch: [2/5][1750/1797] Data 0.007 (0.006) Batch 0.543 (0.543) Remain 00:49:13 MainLoss 0.8660 AuxLoss 0.9420 Loss 1.2428 Accuracy 0.6374.
[2025-02-11 05:55:58,877] INFO: Epoch: [2/5][1760/1797] Data 0.006 (0.006) Batch 0.543 (0.543) Remain 00:49:08 MainLoss 0.9823 AuxLoss 0.9441 Loss 1.3600 Accuracy 0.5900.
[2025-02-11 05:56:04,309] INFO: Epoch: [2/5][1770/1797] Data 0.007 (0.006) Batch 0.543 (0.543) Remain 00:49:02 MainLoss 0.6858 AuxLoss 0.9650 Loss 1.0718 Accuracy 0.7574.
[2025-02-11 05:56:09,739] INFO: Epoch: [2/5][1780/1797] Data 0.006 (0.006) Batch 0.542 (0.543) Remain 00:48:57 MainLoss 0.5763 AuxLoss 0.5175 Loss 0.7833 Accuracy 0.8383.
[2025-02-11 05:56:15,177] INFO: Epoch: [2/5][1790/1797] Data 0.006 (0.006) Batch 0.544 (0.543) Remain 00:48:52 MainLoss 0.5875 AuxLoss 0.7560 Loss 0.8899 Accuracy 0.8076.
[2025-02-11 05:56:19,015] INFO: Train result at epoch [2/5]: mIoU/mAcc/allAcc 0.4537/0.5707/0.7372.
[2025-02-11 05:56:19,017] INFO: Saving checkpoint to: exp/RescueNet/pspnet101/model/train_epoch_s1025_2.pth
[2025-02-11 05:56:25,374] INFO: Epoch: [3/5][10/1797] Data 0.006 (0.039) Batch 0.543 (0.576) Remain 00:51:37 MainLoss 0.5051 AuxLoss 0.5488 Loss 0.7246 Accuracy 0.8370.
[2025-02-11 05:56:30,809] INFO: Epoch: [3/5][20/1797] Data 0.006 (0.022) Batch 0.543 (0.560) Remain 00:50:05 MainLoss 1.0546 AuxLoss 0.9720 Loss 1.4434 Accuracy 0.6085.
[2025-02-11 05:56:36,241] INFO: Epoch: [3/5][30/1797] Data 0.006 (0.017) Batch 0.544 (0.554) Remain 00:49:30 MainLoss 0.3950 AuxLoss 0.4829 Loss 0.5881 Accuracy 0.8499.
[2025-02-11 05:56:41,683] INFO: Epoch: [3/5][40/1797] Data 0.006 (0.014) Batch 0.545 (0.552) Remain 00:49:11 MainLoss 0.4070 AuxLoss 0.4208 Loss 0.5753 Accuracy 0.8420.
[2025-02-11 05:56:47,124] INFO: Epoch: [3/5][50/1797] Data 0.006 (0.013) Batch 0.544 (0.550) Remain 00:48:58 MainLoss 0.5827 AuxLoss 0.5927 Loss 0.8198 Accuracy 0.7796.
[2025-02-11 05:56:52,565] INFO: Epoch: [3/5][60/1797] Data 0.006 (0.012) Batch 0.543 (0.549) Remain 00:48:47 MainLoss 0.5258 AuxLoss 0.5248 Loss 0.7357 Accuracy 0.7814.
[2025-02-11 05:56:58,004] INFO: Epoch: [3/5][70/1797] Data 0.006 (0.011) Batch 0.545 (0.548) Remain 00:48:37 MainLoss 0.7741 AuxLoss 0.8263 Loss 1.1046 Accuracy 0.6843.
[2025-02-11 05:57:03,445] INFO: Epoch: [3/5][80/1797] Data 0.007 (0.010) Batch 0.545 (0.548) Remain 00:48:29 MainLoss 0.5364 AuxLoss 0.5890 Loss 0.7720 Accuracy 0.8290.
[2025-02-11 05:57:08,880] INFO: Epoch: [3/5][90/1797] Data 0.006 (0.010) Batch 0.544 (0.547) Remain 00:48:21 MainLoss 0.2910 AuxLoss 0.3589 Loss 0.4345 Accuracy 0.8780.
[2025-02-11 05:57:14,317] INFO: Epoch: [3/5][100/1797] Data 0.006 (0.009) Batch 0.543 (0.547) Remain 00:48:14 MainLoss 0.2520 AuxLoss 0.3764 Loss 0.4025 Accuracy 0.8919.
[2025-02-11 05:57:19,754] INFO: Epoch: [3/5][110/1797] Data 0.006 (0.009) Batch 0.543 (0.547) Remain 00:48:07 MainLoss 1.8420 AuxLoss 1.7095 Loss 2.5258 Accuracy 0.5813.
[2025-02-11 05:57:25,190] INFO: Epoch: [3/5][120/1797] Data 0.006 (0.009) Batch 0.543 (0.546) Remain 00:48:00 MainLoss 0.5664 AuxLoss 0.5073 Loss 0.7694 Accuracy 0.8034.
[2025-02-11 05:57:30,623] INFO: Epoch: [3/5][130/1797] Data 0.006 (0.009) Batch 0.543 (0.546) Remain 00:47:53 MainLoss 0.8230 AuxLoss 0.8370 Loss 1.1578 Accuracy 0.7456.
[2025-02-11 05:57:36,058] INFO: Epoch: [3/5][140/1797] Data 0.006 (0.009) Batch 0.544 (0.546) Remain 00:47:47 MainLoss 0.4060 AuxLoss 0.7780 Loss 0.7172 Accuracy 0.8590.
[2025-02-11 05:57:41,492] INFO: Epoch: [3/5][150/1797] Data 0.006 (0.008) Batch 0.543 (0.546) Remain 00:47:40 MainLoss 0.2111 AuxLoss 0.4604 Loss 0.3953 Accuracy 0.9738.
[2025-02-11 05:57:46,928] INFO: Epoch: [3/5][160/1797] Data 0.006 (0.008) Batch 0.545 (0.546) Remain 00:47:34 MainLoss 0.2342 AuxLoss 0.4019 Loss 0.3950 Accuracy 0.9403.
[2025-02-11 05:57:52,362] INFO: Epoch: [3/5][170/1797] Data 0.006 (0.008) Batch 0.543 (0.546) Remain 00:47:28 MainLoss 0.6053 AuxLoss 0.6631 Loss 0.8706 Accuracy 0.7801.
[2025-02-11 05:57:57,795] INFO: Epoch: [3/5][180/1797] Data 0.006 (0.008) Batch 0.543 (0.545) Remain 00:47:22 MainLoss 0.6416 AuxLoss 0.6304 Loss 0.8937 Accuracy 0.7444.
[2025-02-11 05:58:03,233] INFO: Epoch: [3/5][190/1797] Data 0.006 (0.008) Batch 0.544 (0.545) Remain 00:47:16 MainLoss 0.5047 AuxLoss 0.6318 Loss 0.7574 Accuracy 0.8154.
[2025-02-11 05:58:08,670] INFO: Epoch: [3/5][200/1797] Data 0.006 (0.008) Batch 0.543 (0.545) Remain 00:47:10 MainLoss 0.5869 AuxLoss 0.7997 Loss 0.9068 Accuracy 0.7891.
[2025-02-11 05:58:14,102] INFO: Epoch: [3/5][210/1797] Data 0.006 (0.008) Batch 0.544 (0.545) Remain 00:47:04 MainLoss 0.4549 AuxLoss 0.4202 Loss 0.6230 Accuracy 0.8894.
[2025-02-11 05:58:19,540] INFO: Epoch: [3/5][220/1797] Data 0.006 (0.008) Batch 0.543 (0.545) Remain 00:46:58 MainLoss 0.5874 AuxLoss 0.7466 Loss 0.8860 Accuracy 0.7329.
[2025-02-11 05:58:25,155] INFO: Epoch: [3/5][230/1797] Data 0.006 (0.008) Batch 0.544 (0.546) Remain 00:46:56 MainLoss 0.5409 AuxLoss 0.5998 Loss 0.7809 Accuracy 0.7060.
[2025-02-11 05:58:30,588] INFO: Epoch: [3/5][240/1797] Data 0.006 (0.008) Batch 0.544 (0.546) Remain 00:46:50 MainLoss 0.5477 AuxLoss 0.5106 Loss 0.7519 Accuracy 0.7661.
[2025-02-11 05:58:36,018] INFO: Epoch: [3/5][250/1797] Data 0.006 (0.008) Batch 0.543 (0.546) Remain 00:46:44 MainLoss 0.4775 AuxLoss 0.6162 Loss 0.7240 Accuracy 0.8483.
[2025-02-11 05:58:41,454] INFO: Epoch: [3/5][260/1797] Data 0.006 (0.008) Batch 0.544 (0.546) Remain 00:46:39 MainLoss 0.6786 AuxLoss 0.8195 Loss 1.0064 Accuracy 0.7927.
[2025-02-11 05:58:46,888] INFO: Epoch: [3/5][270/1797] Data 0.006 (0.008) Batch 0.543 (0.545) Remain 00:46:33 MainLoss 0.5137 AuxLoss 1.1694 Loss 0.9815 Accuracy 0.7986.
[2025-02-11 05:58:52,322] INFO: Epoch: [3/5][280/1797] Data 0.006 (0.008) Batch 0.545 (0.545) Remain 00:46:27 MainLoss 0.1699 AuxLoss 0.2155 Loss 0.2561 Accuracy 0.9597.
[2025-02-11 05:58:57,758] INFO: Epoch: [3/5][290/1797] Data 0.006 (0.008) Batch 0.543 (0.545) Remain 00:46:21 MainLoss 0.8638 AuxLoss 0.8620 Loss 1.2086 Accuracy 0.6472.
[2025-02-11 05:59:03,197] INFO: Epoch: [3/5][300/1797] Data 0.006 (0.008) Batch 0.544 (0.545) Remain 00:46:15 MainLoss 0.5879 AuxLoss 0.5057 Loss 0.7902 Accuracy 0.6327.
[2025-02-11 05:59:08,639] INFO: Epoch: [3/5][310/1797] Data 0.006 (0.008) Batch 0.542 (0.545) Remain 00:46:10 MainLoss 0.8647 AuxLoss 0.9857 Loss 1.2589 Accuracy 0.6591.
[2025-02-11 05:59:14,074] INFO: Epoch: [3/5][320/1797] Data 0.006 (0.008) Batch 0.544 (0.545) Remain 00:46:04 MainLoss 0.4938 AuxLoss 0.6518 Loss 0.7545 Accuracy 0.8466.
[2025-02-11 05:59:19,503] INFO: Epoch: [3/5][330/1797] Data 0.006 (0.008) Batch 0.542 (0.545) Remain 00:45:58 MainLoss 0.3387 AuxLoss 0.3498 Loss 0.4786 Accuracy 0.8994.
[2025-02-11 05:59:24,937] INFO: Epoch: [3/5][340/1797] Data 0.006 (0.008) Batch 0.542 (0.545) Remain 00:45:53 MainLoss 0.6250 AuxLoss 0.8775 Loss 0.9760 Accuracy 0.7804.
[2025-02-11 05:59:30,368] INFO: Epoch: [3/5][350/1797] Data 0.006 (0.008) Batch 0.543 (0.545) Remain 00:45:47 MainLoss 0.7723 AuxLoss 1.0629 Loss 1.1974 Accuracy 0.6850.
[2025-02-11 05:59:35,796] INFO: Epoch: [3/5][360/1797] Data 0.006 (0.008) Batch 0.542 (0.545) Remain 00:45:41 MainLoss 0.6724 AuxLoss 0.8953 Loss 1.0306 Accuracy 0.8071.
[2025-02-11 05:59:41,239] INFO: Epoch: [3/5][370/1797] Data 0.006 (0.008) Batch 0.543 (0.545) Remain 00:45:36 MainLoss 0.5542 AuxLoss 0.7515 Loss 0.8548 Accuracy 0.8200.
[2025-02-11 05:59:46,668] INFO: Epoch: [3/5][380/1797] Data 0.006 (0.008) Batch 0.543 (0.545) Remain 00:45:30 MainLoss 0.5521 AuxLoss 0.6709 Loss 0.8204 Accuracy 0.7943.
[2025-02-11 05:59:52,098] INFO: Epoch: [3/5][390/1797] Data 0.006 (0.008) Batch 0.543 (0.545) Remain 00:45:24 MainLoss 0.9043 AuxLoss 0.8495 Loss 1.2441 Accuracy 0.6302.
[2025-02-11 05:59:57,541] INFO: Epoch: [3/5][400/1797] Data 0.006 (0.008) Batch 0.542 (0.545) Remain 00:45:19 MainLoss 0.4198 AuxLoss 0.5150 Loss 0.6258 Accuracy 0.8118.
[2025-02-11 06:00:02,979] INFO: Epoch: [3/5][410/1797] Data 0.006 (0.008) Batch 0.543 (0.545) Remain 00:45:13 MainLoss 0.4744 AuxLoss 0.6580 Loss 0.7376 Accuracy 0.8151.
[2025-02-11 06:00:08,417] INFO: Epoch: [3/5][420/1797] Data 0.006 (0.008) Batch 0.542 (0.545) Remain 00:45:08 MainLoss 0.4910 AuxLoss 0.7104 Loss 0.7751 Accuracy 0.7946.
[2025-02-11 06:00:13,859] INFO: Epoch: [3/5][430/1797] Data 0.006 (0.008) Batch 0.545 (0.545) Remain 00:45:02 MainLoss 0.2868 AuxLoss 0.4158 Loss 0.4532 Accuracy 0.8941.
[2025-02-11 06:00:19,298] INFO: Epoch: [3/5][440/1797] Data 0.006 (0.007) Batch 0.543 (0.545) Remain 00:44:56 MainLoss 1.3099 AuxLoss 1.2088 Loss 1.7934 Accuracy 0.5085.
[2025-02-11 06:00:24,751] INFO: Epoch: [3/5][450/1797] Data 0.006 (0.007) Batch 0.544 (0.545) Remain 00:44:51 MainLoss 0.7702 AuxLoss 0.8168 Loss 1.0969 Accuracy 0.6814.
[2025-02-11 06:00:30,194] INFO: Epoch: [3/5][460/1797] Data 0.006 (0.007) Batch 0.544 (0.545) Remain 00:44:46 MainLoss 0.2404 AuxLoss 0.3649 Loss 0.3864 Accuracy 0.9139.
[2025-02-11 06:00:35,630] INFO: Epoch: [3/5][470/1797] Data 0.006 (0.007) Batch 0.545 (0.545) Remain 00:44:40 MainLoss 0.4413 AuxLoss 0.7137 Loss 0.7268 Accuracy 0.8649.
[2025-02-11 06:00:41,067] INFO: Epoch: [3/5][480/1797] Data 0.006 (0.007) Batch 0.543 (0.545) Remain 00:44:34 MainLoss 0.1492 AuxLoss 0.2168 Loss 0.2359 Accuracy 0.9715.
[2025-02-11 06:00:46,505] INFO: Epoch: [3/5][490/1797] Data 0.006 (0.007) Batch 0.544 (0.545) Remain 00:44:29 MainLoss 0.6287 AuxLoss 0.7595 Loss 0.9325 Accuracy 0.8148.
[2025-02-11 06:00:51,943] INFO: Epoch: [3/5][500/1797] Data 0.006 (0.007) Batch 0.543 (0.545) Remain 00:44:23 MainLoss 0.6018 AuxLoss 0.8080 Loss 0.9250 Accuracy 0.7216.
[2025-02-11 06:00:57,379] INFO: Epoch: [3/5][510/1797] Data 0.006 (0.007) Batch 0.543 (0.545) Remain 00:44:18 MainLoss 1.5961 AuxLoss 1.1842 Loss 2.0698 Accuracy 0.5630.
[2025-02-11 06:01:02,820] INFO: Epoch: [3/5][520/1797] Data 0.007 (0.007) Batch 0.545 (0.545) Remain 00:44:12 MainLoss 0.7032 AuxLoss 1.2112 Loss 1.1877 Accuracy 0.6958.
[2025-02-11 06:01:08,260] INFO: Epoch: [3/5][530/1797] Data 0.006 (0.007) Batch 0.545 (0.545) Remain 00:44:07 MainLoss 0.5126 AuxLoss 0.7253 Loss 0.8028 Accuracy 0.8129.
[2025-02-11 06:01:13,699] INFO: Epoch: [3/5][540/1797] Data 0.006 (0.007) Batch 0.545 (0.545) Remain 00:44:01 MainLoss 0.2665 AuxLoss 0.4027 Loss 0.4276 Accuracy 0.9265.
[2025-02-11 06:01:19,147] INFO: Epoch: [3/5][550/1797] Data 0.006 (0.007) Batch 0.544 (0.545) Remain 00:43:56 MainLoss 0.2926 AuxLoss 0.4214 Loss 0.4612 Accuracy 0.9308.
[2025-02-11 06:01:24,586] INFO: Epoch: [3/5][560/1797] Data 0.006 (0.007) Batch 0.542 (0.545) Remain 00:43:50 MainLoss 0.6856 AuxLoss 0.7630 Loss 0.9908 Accuracy 0.8203.
[2025-02-11 06:01:30,024] INFO: Epoch: [3/5][570/1797] Data 0.006 (0.007) Batch 0.543 (0.545) Remain 00:43:45 MainLoss 0.9353 AuxLoss 0.8904 Loss 1.2914 Accuracy 0.6071.
[2025-02-11 06:01:35,460] INFO: Epoch: [3/5][580/1797] Data 0.006 (0.007) Batch 0.544 (0.545) Remain 00:43:39 MainLoss 0.6350 AuxLoss 0.7242 Loss 0.9246 Accuracy 0.7147.
[2025-02-11 06:01:40,893] INFO: Epoch: [3/5][590/1797] Data 0.006 (0.007) Batch 0.544 (0.545) Remain 00:43:34 MainLoss 0.6866 AuxLoss 0.7648 Loss 0.9926 Accuracy 0.7568.
[2025-02-11 06:01:46,327] INFO: Epoch: [3/5][600/1797] Data 0.006 (0.007) Batch 0.543 (0.545) Remain 00:43:28 MainLoss 0.5295 AuxLoss 0.5303 Loss 0.7417 Accuracy 0.7977.
[2025-02-11 06:01:51,761] INFO: Epoch: [3/5][610/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:43:23 MainLoss 0.7425 AuxLoss 0.8207 Loss 1.0708 Accuracy 0.7360.
[2025-02-11 06:01:57,192] INFO: Epoch: [3/5][620/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:43:17 MainLoss 1.9255 AuxLoss 1.4873 Loss 2.5204 Accuracy 0.5641.
[2025-02-11 06:02:02,623] INFO: Epoch: [3/5][630/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:43:12 MainLoss 0.7218 AuxLoss 0.6909 Loss 0.9981 Accuracy 0.6309.
[2025-02-11 06:02:08,056] INFO: Epoch: [3/5][640/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:43:06 MainLoss 1.0634 AuxLoss 1.0225 Loss 1.4724 Accuracy 0.6132.
[2025-02-11 06:02:13,490] INFO: Epoch: [3/5][650/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:43:01 MainLoss 0.5696 AuxLoss 0.7753 Loss 0.8797 Accuracy 0.7788.
[2025-02-11 06:02:18,923] INFO: Epoch: [3/5][660/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:42:55 MainLoss 0.6664 AuxLoss 0.8737 Loss 1.0159 Accuracy 0.7592.
[2025-02-11 06:02:24,360] INFO: Epoch: [3/5][670/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:42:50 MainLoss 0.6141 AuxLoss 0.8385 Loss 0.9495 Accuracy 0.7123.
[2025-02-11 06:02:29,793] INFO: Epoch: [3/5][680/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:42:44 MainLoss 0.8135 AuxLoss 0.9636 Loss 1.1989 Accuracy 0.7173.
[2025-02-11 06:02:35,230] INFO: Epoch: [3/5][690/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:42:39 MainLoss 0.8923 AuxLoss 0.8547 Loss 1.2342 Accuracy 0.6417.
[2025-02-11 06:02:40,668] INFO: Epoch: [3/5][700/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:42:33 MainLoss 0.9505 AuxLoss 0.9288 Loss 1.3220 Accuracy 0.5494.
[2025-02-11 06:02:46,102] INFO: Epoch: [3/5][710/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:42:28 MainLoss 0.3701 AuxLoss 0.7704 Loss 0.6783 Accuracy 0.8592.
[2025-02-11 06:02:51,533] INFO: Epoch: [3/5][720/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:42:22 MainLoss 0.9383 AuxLoss 0.7676 Loss 1.2453 Accuracy 0.6736.
[2025-02-11 06:02:56,960] INFO: Epoch: [3/5][730/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:42:17 MainLoss 0.8576 AuxLoss 1.0338 Loss 1.2711 Accuracy 0.5375.
[2025-02-11 06:03:02,393] INFO: Epoch: [3/5][740/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:42:11 MainLoss 0.5175 AuxLoss 0.7271 Loss 0.8083 Accuracy 0.8423.
[2025-02-11 06:03:07,822] INFO: Epoch: [3/5][750/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:42:05 MainLoss 0.5838 AuxLoss 0.5989 Loss 0.8234 Accuracy 0.7447.
[2025-02-11 06:03:13,254] INFO: Epoch: [3/5][760/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:42:00 MainLoss 0.6404 AuxLoss 0.8482 Loss 0.9797 Accuracy 0.7751.
[2025-02-11 06:03:18,686] INFO: Epoch: [3/5][770/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:41:54 MainLoss 0.4276 AuxLoss 0.4530 Loss 0.6088 Accuracy 0.8824.
[2025-02-11 06:03:24,120] INFO: Epoch: [3/5][780/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:41:49 MainLoss 0.6962 AuxLoss 0.6654 Loss 0.9623 Accuracy 0.7069.
[2025-02-11 06:03:29,553] INFO: Epoch: [3/5][790/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:41:43 MainLoss 0.1654 AuxLoss 0.2117 Loss 0.2501 Accuracy 0.9546.
[2025-02-11 06:03:34,983] INFO: Epoch: [3/5][800/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:41:38 MainLoss 0.6933 AuxLoss 0.7453 Loss 0.9914 Accuracy 0.7175.
[2025-02-11 06:03:40,416] INFO: Epoch: [3/5][810/1797] Data 0.006 (0.007) Batch 0.545 (0.544) Remain 00:41:32 MainLoss 0.4217 AuxLoss 0.4008 Loss 0.5821 Accuracy 0.8752.
[2025-02-11 06:03:45,844] INFO: Epoch: [3/5][820/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:41:27 MainLoss 0.6524 AuxLoss 0.8346 Loss 0.9863 Accuracy 0.7857.
[2025-02-11 06:03:51,276] INFO: Epoch: [3/5][830/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:41:21 MainLoss 0.7453 AuxLoss 0.8678 Loss 1.0924 Accuracy 0.7324.
[2025-02-11 06:03:56,713] INFO: Epoch: [3/5][840/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:41:16 MainLoss 0.4379 AuxLoss 0.5444 Loss 0.6557 Accuracy 0.8458.
[2025-02-11 06:04:02,140] INFO: Epoch: [3/5][850/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:41:10 MainLoss 0.6885 AuxLoss 0.8356 Loss 1.0227 Accuracy 0.7737.
[2025-02-11 06:04:07,567] INFO: Epoch: [3/5][860/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:41:05 MainLoss 0.6689 AuxLoss 0.8443 Loss 1.0066 Accuracy 0.8074.
[2025-02-11 06:04:12,993] INFO: Epoch: [3/5][870/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:40:59 MainLoss 0.4864 AuxLoss 0.5812 Loss 0.7189 Accuracy 0.7764.
[2025-02-11 06:04:18,421] INFO: Epoch: [3/5][880/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:40:54 MainLoss 0.3840 AuxLoss 0.4375 Loss 0.5591 Accuracy 0.9036.
[2025-02-11 06:04:23,850] INFO: Epoch: [3/5][890/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:40:48 MainLoss 0.3797 AuxLoss 0.3808 Loss 0.5320 Accuracy 0.9268.
[2025-02-11 06:04:29,282] INFO: Epoch: [3/5][900/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:40:43 MainLoss 0.4510 AuxLoss 0.4161 Loss 0.6175 Accuracy 0.8536.
[2025-02-11 06:04:34,711] INFO: Epoch: [3/5][910/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:40:37 MainLoss 1.0268 AuxLoss 0.8653 Loss 1.3729 Accuracy 0.5655.
[2025-02-11 06:04:40,145] INFO: Epoch: [3/5][920/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:40:32 MainLoss 0.5343 AuxLoss 0.6362 Loss 0.7888 Accuracy 0.8215.
[2025-02-11 06:04:45,579] INFO: Epoch: [3/5][930/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:40:26 MainLoss 0.3171 AuxLoss 0.5156 Loss 0.5233 Accuracy 0.9217.
[2025-02-11 06:04:51,010] INFO: Epoch: [3/5][940/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:40:21 MainLoss 0.2666 AuxLoss 0.5552 Loss 0.4887 Accuracy 0.9298.
[2025-02-11 06:04:56,439] INFO: Epoch: [3/5][950/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:40:16 MainLoss 0.8427 AuxLoss 1.1545 Loss 1.3045 Accuracy 0.6949.
[2025-02-11 06:05:01,868] INFO: Epoch: [3/5][960/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:40:10 MainLoss 0.4859 AuxLoss 0.5793 Loss 0.7176 Accuracy 0.8950.
[2025-02-11 06:05:07,301] INFO: Epoch: [3/5][970/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:40:05 MainLoss 0.3402 AuxLoss 0.4538 Loss 0.5217 Accuracy 0.9258.
[2025-02-11 06:05:12,736] INFO: Epoch: [3/5][980/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:39:59 MainLoss 0.3522 AuxLoss 0.4380 Loss 0.5275 Accuracy 0.9144.
[2025-02-11 06:05:18,166] INFO: Epoch: [3/5][990/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:39:54 MainLoss 0.6963 AuxLoss 0.7645 Loss 1.0021 Accuracy 0.7173.
[2025-02-11 06:05:23,595] INFO: Epoch: [3/5][1000/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:39:48 MainLoss 1.1659 AuxLoss 1.0207 Loss 1.5741 Accuracy 0.5310.
[2025-02-11 06:05:29,029] INFO: Epoch: [3/5][1010/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:39:43 MainLoss 0.4662 AuxLoss 0.5518 Loss 0.6870 Accuracy 0.8088.
[2025-02-11 06:05:34,462] INFO: Epoch: [3/5][1020/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:39:37 MainLoss 0.4223 AuxLoss 0.5185 Loss 0.6297 Accuracy 0.8783.
[2025-02-11 06:05:39,898] INFO: Epoch: [3/5][1030/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:39:32 MainLoss 0.6390 AuxLoss 0.6888 Loss 0.9145 Accuracy 0.7611.
[2025-02-11 06:05:45,339] INFO: Epoch: [3/5][1040/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:39:26 MainLoss 1.0251 AuxLoss 1.3005 Loss 1.5453 Accuracy 0.7279.
[2025-02-11 06:05:50,765] INFO: Epoch: [3/5][1050/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:39:21 MainLoss 1.1793 AuxLoss 1.0512 Loss 1.5998 Accuracy 0.5416.
[2025-02-11 06:05:56,198] INFO: Epoch: [3/5][1060/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:39:15 MainLoss 0.9652 AuxLoss 0.9206 Loss 1.3335 Accuracy 0.6071.
[2025-02-11 06:06:01,636] INFO: Epoch: [3/5][1070/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:39:10 MainLoss 0.7232 AuxLoss 0.9866 Loss 1.1179 Accuracy 0.7259.
[2025-02-11 06:06:07,062] INFO: Epoch: [3/5][1080/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:39:04 MainLoss 0.3976 AuxLoss 0.5058 Loss 0.5999 Accuracy 0.8744.
[2025-02-11 06:06:12,494] INFO: Epoch: [3/5][1090/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:38:59 MainLoss 0.4969 AuxLoss 0.5811 Loss 0.7293 Accuracy 0.7742.
[2025-02-11 06:06:17,926] INFO: Epoch: [3/5][1100/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:38:53 MainLoss 0.6331 AuxLoss 1.1251 Loss 1.0831 Accuracy 0.7448.
[2025-02-11 06:06:23,359] INFO: Epoch: [3/5][1110/1797] Data 0.007 (0.007) Batch 0.544 (0.544) Remain 00:38:48 MainLoss 0.4650 AuxLoss 0.5578 Loss 0.6881 Accuracy 0.8574.
[2025-02-11 06:06:28,793] INFO: Epoch: [3/5][1120/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:38:43 MainLoss 0.5400 AuxLoss 0.7132 Loss 0.8253 Accuracy 0.8167.
[2025-02-11 06:06:34,232] INFO: Epoch: [3/5][1130/1797] Data 0.006 (0.007) Batch 0.546 (0.544) Remain 00:38:37 MainLoss 0.6491 AuxLoss 0.7185 Loss 0.9364 Accuracy 0.7829.
[2025-02-11 06:06:39,668] INFO: Epoch: [3/5][1140/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:38:32 MainLoss 0.9040 AuxLoss 0.9818 Loss 1.2967 Accuracy 0.6185.
[2025-02-11 06:06:45,111] INFO: Epoch: [3/5][1150/1797] Data 0.007 (0.007) Batch 0.544 (0.544) Remain 00:38:26 MainLoss 0.5348 AuxLoss 0.5927 Loss 0.7719 Accuracy 0.8119.
[2025-02-11 06:06:50,545] INFO: Epoch: [3/5][1160/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:38:21 MainLoss 0.2133 AuxLoss 0.3192 Loss 0.3410 Accuracy 0.9267.
[2025-02-11 06:06:55,972] INFO: Epoch: [3/5][1170/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:38:15 MainLoss 0.4116 AuxLoss 0.6327 Loss 0.6647 Accuracy 0.8411.
[2025-02-11 06:07:01,409] INFO: Epoch: [3/5][1180/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:38:10 MainLoss 0.6203 AuxLoss 0.7515 Loss 0.9209 Accuracy 0.7686.
[2025-02-11 06:07:06,835] INFO: Epoch: [3/5][1190/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:38:04 MainLoss 0.7474 AuxLoss 0.8553 Loss 1.0895 Accuracy 0.5996.
[2025-02-11 06:07:12,266] INFO: Epoch: [3/5][1200/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:37:59 MainLoss 0.7474 AuxLoss 1.1580 Loss 1.2106 Accuracy 0.6815.
[2025-02-11 06:07:17,700] INFO: Epoch: [3/5][1210/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:37:53 MainLoss 0.7511 AuxLoss 0.9163 Loss 1.1177 Accuracy 0.7350.
[2025-02-11 06:07:23,136] INFO: Epoch: [3/5][1220/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:37:48 MainLoss 0.5907 AuxLoss 0.8108 Loss 0.9150 Accuracy 0.7709.
[2025-02-11 06:07:28,573] INFO: Epoch: [3/5][1230/1797] Data 0.007 (0.007) Batch 0.543 (0.544) Remain 00:37:43 MainLoss 0.4813 AuxLoss 0.6548 Loss 0.7432 Accuracy 0.8621.
[2025-02-11 06:07:34,007] INFO: Epoch: [3/5][1240/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:37:37 MainLoss 0.6131 AuxLoss 0.6742 Loss 0.8828 Accuracy 0.7958.
[2025-02-11 06:07:39,443] INFO: Epoch: [3/5][1250/1797] Data 0.006 (0.007) Batch 0.546 (0.544) Remain 00:37:32 MainLoss 0.2364 AuxLoss 0.2520 Loss 0.3372 Accuracy 0.9412.
[2025-02-11 06:07:44,877] INFO: Epoch: [3/5][1260/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:37:26 MainLoss 0.4775 AuxLoss 0.5074 Loss 0.6805 Accuracy 0.8485.
[2025-02-11 06:07:50,311] INFO: Epoch: [3/5][1270/1797] Data 0.006 (0.007) Batch 0.545 (0.544) Remain 00:37:21 MainLoss 0.6539 AuxLoss 0.7027 Loss 0.9350 Accuracy 0.6877.
[2025-02-11 06:07:55,750] INFO: Epoch: [3/5][1280/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:37:15 MainLoss 0.6686 AuxLoss 0.7935 Loss 0.9860 Accuracy 0.7968.
[2025-02-11 06:08:01,183] INFO: Epoch: [3/5][1290/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:37:10 MainLoss 0.6759 AuxLoss 0.8906 Loss 1.0321 Accuracy 0.7446.
[2025-02-11 06:08:06,612] INFO: Epoch: [3/5][1300/1797] Data 0.006 (0.007) Batch 0.541 (0.544) Remain 00:37:04 MainLoss 1.0412 AuxLoss 0.9338 Loss 1.4147 Accuracy 0.6418.
[2025-02-11 06:08:12,051] INFO: Epoch: [3/5][1310/1797] Data 0.006 (0.007) Batch 0.545 (0.544) Remain 00:36:59 MainLoss 0.3203 AuxLoss 0.3136 Loss 0.4457 Accuracy 0.9059.
[2025-02-11 06:08:17,807] INFO: Epoch: [3/5][1320/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:36:54 MainLoss 0.4199 AuxLoss 0.6304 Loss 0.6721 Accuracy 0.9160.
[2025-02-11 06:08:23,242] INFO: Epoch: [3/5][1330/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:36:49 MainLoss 0.4208 AuxLoss 0.4708 Loss 0.6091 Accuracy 0.8903.
[2025-02-11 06:08:28,675] INFO: Epoch: [3/5][1340/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:36:44 MainLoss 0.9360 AuxLoss 1.2236 Loss 1.4254 Accuracy 0.5598.
[2025-02-11 06:08:34,109] INFO: Epoch: [3/5][1350/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:36:38 MainLoss 0.1093 AuxLoss 0.3938 Loss 0.2668 Accuracy 0.9854.
[2025-02-11 06:08:39,547] INFO: Epoch: [3/5][1360/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:36:33 MainLoss 0.8211 AuxLoss 1.0058 Loss 1.2234 Accuracy 0.6035.
[2025-02-11 06:08:44,982] INFO: Epoch: [3/5][1370/1797] Data 0.006 (0.007) Batch 0.545 (0.544) Remain 00:36:27 MainLoss 0.3036 AuxLoss 0.4312 Loss 0.4761 Accuracy 0.9186.
[2025-02-11 06:08:50,418] INFO: Epoch: [3/5][1380/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:36:22 MainLoss 0.8612 AuxLoss 0.9092 Loss 1.2249 Accuracy 0.6760.
[2025-02-11 06:08:55,856] INFO: Epoch: [3/5][1390/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:36:16 MainLoss 0.2528 AuxLoss 0.9533 Loss 0.6341 Accuracy 0.9630.
[2025-02-11 06:09:01,299] INFO: Epoch: [3/5][1400/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:36:11 MainLoss 0.7492 AuxLoss 0.8465 Loss 1.0878 Accuracy 0.7025.
[2025-02-11 06:09:06,745] INFO: Epoch: [3/5][1410/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:36:05 MainLoss 0.4462 AuxLoss 0.6975 Loss 0.7252 Accuracy 0.8858.
[2025-02-11 06:09:12,185] INFO: Epoch: [3/5][1420/1797] Data 0.007 (0.007) Batch 0.544 (0.544) Remain 00:36:00 MainLoss 0.4172 AuxLoss 0.5155 Loss 0.6233 Accuracy 0.8966.
[2025-02-11 06:09:17,633] INFO: Epoch: [3/5][1430/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:35:55 MainLoss 0.1743 AuxLoss 0.3520 Loss 0.3151 Accuracy 0.9488.
[2025-02-11 06:09:23,075] INFO: Epoch: [3/5][1440/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:35:49 MainLoss 0.5858 AuxLoss 0.7246 Loss 0.8757 Accuracy 0.7228.
[2025-02-11 06:09:28,518] INFO: Epoch: [3/5][1450/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:35:44 MainLoss 0.9961 AuxLoss 1.0194 Loss 1.4039 Accuracy 0.6022.
[2025-02-11 06:09:33,958] INFO: Epoch: [3/5][1460/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:35:38 MainLoss 0.4620 AuxLoss 0.6819 Loss 0.7347 Accuracy 0.8153.
[2025-02-11 06:09:39,400] INFO: Epoch: [3/5][1470/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:35:33 MainLoss 0.6646 AuxLoss 0.8002 Loss 0.9847 Accuracy 0.7419.
[2025-02-11 06:09:44,841] INFO: Epoch: [3/5][1480/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:35:27 MainLoss 0.6782 AuxLoss 1.6841 Loss 1.3518 Accuracy 0.8304.
[2025-02-11 06:09:50,280] INFO: Epoch: [3/5][1490/1797] Data 0.007 (0.007) Batch 0.544 (0.544) Remain 00:35:22 MainLoss 0.3059 AuxLoss 0.5381 Loss 0.5212 Accuracy 0.9097.
[2025-02-11 06:09:55,715] INFO: Epoch: [3/5][1500/1797] Data 0.007 (0.007) Batch 0.543 (0.544) Remain 00:35:16 MainLoss 2.0188 AuxLoss 1.6902 Loss 2.6949 Accuracy 0.3242.
[2025-02-11 06:10:01,152] INFO: Epoch: [3/5][1510/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:35:11 MainLoss 0.8152 AuxLoss 1.0120 Loss 1.2200 Accuracy 0.5625.
[2025-02-11 06:10:06,588] INFO: Epoch: [3/5][1520/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:35:06 MainLoss 0.7727 AuxLoss 0.7176 Loss 1.0597 Accuracy 0.7377.
[2025-02-11 06:10:12,025] INFO: Epoch: [3/5][1530/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:35:00 MainLoss 0.8711 AuxLoss 0.9412 Loss 1.2475 Accuracy 0.6902.
[2025-02-11 06:10:17,466] INFO: Epoch: [3/5][1540/1797] Data 0.006 (0.007) Batch 0.545 (0.544) Remain 00:34:55 MainLoss 0.7549 AuxLoss 0.7815 Loss 1.0675 Accuracy 0.7357.
[2025-02-11 06:10:22,904] INFO: Epoch: [3/5][1550/1797] Data 0.007 (0.007) Batch 0.545 (0.544) Remain 00:34:49 MainLoss 0.2857 AuxLoss 0.4020 Loss 0.4465 Accuracy 0.9392.
[2025-02-11 06:10:28,341] INFO: Epoch: [3/5][1560/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:34:44 MainLoss 0.5186 AuxLoss 0.7021 Loss 0.7995 Accuracy 0.8152.
[2025-02-11 06:10:33,776] INFO: Epoch: [3/5][1570/1797] Data 0.007 (0.007) Batch 0.544 (0.544) Remain 00:34:38 MainLoss 0.3937 AuxLoss 0.5078 Loss 0.5968 Accuracy 0.8586.
[2025-02-11 06:10:39,211] INFO: Epoch: [3/5][1580/1797] Data 0.006 (0.007) Batch 0.545 (0.544) Remain 00:34:33 MainLoss 0.4847 AuxLoss 0.6114 Loss 0.7292 Accuracy 0.7874.
[2025-02-11 06:10:44,644] INFO: Epoch: [3/5][1590/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:34:27 MainLoss 0.4210 AuxLoss 0.5936 Loss 0.6585 Accuracy 0.8113.
[2025-02-11 06:10:50,073] INFO: Epoch: [3/5][1600/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:34:22 MainLoss 0.7864 AuxLoss 0.8460 Loss 1.1248 Accuracy 0.7010.
[2025-02-11 06:10:55,506] INFO: Epoch: [3/5][1610/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:34:16 MainLoss 0.4362 AuxLoss 0.5207 Loss 0.6445 Accuracy 0.8730.
[2025-02-11 06:11:00,936] INFO: Epoch: [3/5][1620/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:34:11 MainLoss 0.4332 AuxLoss 0.5235 Loss 0.6426 Accuracy 0.8516.
[2025-02-11 06:11:06,365] INFO: Epoch: [3/5][1630/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:34:06 MainLoss 0.2384 AuxLoss 0.3313 Loss 0.3709 Accuracy 0.9497.
[2025-02-11 06:11:11,800] INFO: Epoch: [3/5][1640/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:34:00 MainLoss 0.5350 AuxLoss 0.7079 Loss 0.8182 Accuracy 0.7761.
[2025-02-11 06:11:17,245] INFO: Epoch: [3/5][1650/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:33:55 MainLoss 0.7603 AuxLoss 0.8285 Loss 1.0917 Accuracy 0.6655.
[2025-02-11 06:11:22,677] INFO: Epoch: [3/5][1660/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:33:49 MainLoss 1.4570 AuxLoss 1.7957 Loss 2.1752 Accuracy 0.4074.
[2025-02-11 06:11:28,116] INFO: Epoch: [3/5][1670/1797] Data 0.007 (0.007) Batch 0.543 (0.544) Remain 00:33:44 MainLoss 0.4718 AuxLoss 0.6314 Loss 0.7244 Accuracy 0.8538.
[2025-02-11 06:11:33,552] INFO: Epoch: [3/5][1680/1797] Data 0.007 (0.007) Batch 0.543 (0.544) Remain 00:33:38 MainLoss 0.2689 AuxLoss 0.4610 Loss 0.4533 Accuracy 0.9185.
[2025-02-11 06:11:38,989] INFO: Epoch: [3/5][1690/1797] Data 0.006 (0.007) Batch 0.545 (0.544) Remain 00:33:33 MainLoss 0.8346 AuxLoss 0.6427 Loss 1.0917 Accuracy 0.6807.
[2025-02-11 06:11:44,425] INFO: Epoch: [3/5][1700/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:33:27 MainLoss 0.5211 AuxLoss 0.7084 Loss 0.8045 Accuracy 0.8224.
[2025-02-11 06:11:49,856] INFO: Epoch: [3/5][1710/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:33:22 MainLoss 0.6043 AuxLoss 0.6432 Loss 0.8615 Accuracy 0.8084.
[2025-02-11 06:11:55,296] INFO: Epoch: [3/5][1720/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:33:17 MainLoss 0.5626 AuxLoss 0.5682 Loss 0.7899 Accuracy 0.8218.
[2025-02-11 06:12:00,737] INFO: Epoch: [3/5][1730/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:33:11 MainLoss 0.4642 AuxLoss 0.5030 Loss 0.6654 Accuracy 0.8119.
[2025-02-11 06:12:06,174] INFO: Epoch: [3/5][1740/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:33:06 MainLoss 0.5662 AuxLoss 0.5837 Loss 0.7997 Accuracy 0.7106.
[2025-02-11 06:12:11,609] INFO: Epoch: [3/5][1750/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:33:00 MainLoss 0.3416 AuxLoss 0.4496 Loss 0.5215 Accuracy 0.8988.
[2025-02-11 06:12:17,046] INFO: Epoch: [3/5][1760/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:32:55 MainLoss 1.6156 AuxLoss 1.3328 Loss 2.1487 Accuracy 0.5339.
[2025-02-11 06:12:22,484] INFO: Epoch: [3/5][1770/1797] Data 0.006 (0.007) Batch 0.546 (0.544) Remain 00:32:49 MainLoss 0.3145 AuxLoss 0.5723 Loss 0.5434 Accuracy 0.8956.
[2025-02-11 06:12:27,922] INFO: Epoch: [3/5][1780/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:32:44 MainLoss 0.7186 AuxLoss 0.6995 Loss 0.9984 Accuracy 0.7550.
[2025-02-11 06:12:33,360] INFO: Epoch: [3/5][1790/1797] Data 0.006 (0.007) Batch 0.544 (0.544) Remain 00:32:38 MainLoss 0.3880 AuxLoss 0.2790 Loss 0.4996 Accuracy 0.7940.
[2025-02-11 06:12:37,203] INFO: Train result at epoch [3/5]: mIoU/mAcc/allAcc 0.4935/0.6109/0.7668.
[2025-02-11 06:12:37,208] INFO: Saving checkpoint to: exp/RescueNet/pspnet101/model/train_epoch_s1025_3.pth
[2025-02-11 06:26:04,271] INFO: arch: pspnet
aux_weight: 0.4
base_lr: 0.001
base_size: 2048
batch_size: 2
batch_size_val: 1
classes: 11
data_root: dataset/RescueNet
dataset: rescuenet
dataset_dir: dataset/RescueNet
device: cuda
dist_backend: nccl
dist_url: tcp://127.0.0.1:6789
distributed: False
epochs: 5
evaluate: False
has_prediction: False
ignore_label: 255
ignore_unlabeled: True
imshow_batch: True
index_split: 5
index_start: 0
index_step: 0
keep_batchnorm_fp32: None
layers: 101
loss_scale: None
manual_seed: None
mode: vis
model_path: None
momentum: 0.9
multiprocessing_distributed: False
ngpus_per_node: 1
opt_level: O0
output: outputs
power: 0.9
predict_color: True
print_freq: 100
print_step: False
rank: 0
resume: exp/RescueNet/pspnet101/model/train_epoch_s1025_3.pth
rotate_max: 10
rotate_min: -10
save_folder: None
save_freq: 1
save_path: exp/RescueNet/pspnet101/model
scale_max: 2.0
scale_min: 0.5
scales: [1.0]
split: val
start_epoch: 0
sync_bn: False
test_gpu: [0]
test_h: 713
test_list: dataset/michael/list/test_img_list.txt
test_w: 713
train_gpu: [0]
train_h: 1025
train_list: dataset/cityscapes/list/fine_train.txt
train_w: 1025
use_apex: True
use_pretrained_weights: True
val_list: dataset/cityscapes/list/fine_val.txt
weight: None
weight_decay: 1e-05
workers: 4
world_size: 1
zoom_factor: 8
[2025-02-11 06:26:04,272] INFO: => creating model ...
[2025-02-11 06:26:04,272] INFO: Classes: 11
[2025-02-11 06:26:04,273] INFO: PSPNet(
  (criterion): CrossEntropyLoss()
  (layer0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (6): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (7): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (8): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (9): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (10): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (11): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (12): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (13): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (14): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (15): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (16): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (17): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (18): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (19): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (20): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (21): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (22): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (ppm): PPM(
    (features): ModuleList(
      (0): Sequential(
        (0): AdaptiveAvgPool2d(output_size=1)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): AdaptiveAvgPool2d(output_size=2)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): AdaptiveAvgPool2d(output_size=3)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (3): Sequential(
        (0): AdaptiveAvgPool2d(output_size=6)
        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
    )
  )
  (cls): Sequential(
    (0): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Dropout2d(p=0.1, inplace=False)
    (4): Conv2d(512, 11, kernel_size=(1, 1), stride=(1, 1))
  )
  (aux): Sequential(
    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Dropout2d(p=0.1, inplace=False)
    (4): Conv2d(256, 11, kernel_size=(1, 1), stride=(1, 1))
  )
)
[2025-02-11 06:26:04,410] INFO: => loading checkpoint 'exp/RescueNet/pspnet101/model/train_epoch_s1025_3.pth'
[2025-02-11 06:26:05,386] INFO: => loaded checkpoint 'exp/RescueNet/pspnet101/model/train_epoch_s1025_3.pth' (epoch 3)
[2025-02-11 06:27:00,228] INFO: Epoch: [4/5][100/1797] Data 0.006 (0.010) Batch 0.542 (0.548) Remain 00:31:55 MainLoss 0.7175 AuxLoss 0.7215 Loss 1.0061 Accuracy 0.6617.
[2025-02-11 06:27:54,539] INFO: Epoch: [4/5][200/1797] Data 0.006 (0.008) Batch 0.543 (0.546) Remain 00:30:51 MainLoss 0.2970 AuxLoss 0.4308 Loss 0.4693 Accuracy 0.9216.
[2025-02-11 06:28:48,857] INFO: Epoch: [4/5][300/1797] Data 0.006 (0.007) Batch 0.543 (0.545) Remain 00:29:54 MainLoss 1.0613 AuxLoss 1.0843 Loss 1.4950 Accuracy 0.5330.
[2025-02-11 06:29:43,167] INFO: Epoch: [4/5][400/1797] Data 0.006 (0.007) Batch 0.542 (0.544) Remain 00:28:58 MainLoss 0.5784 AuxLoss 0.6424 Loss 0.8353 Accuracy 0.6949.
[2025-02-11 06:30:37,490] INFO: Epoch: [4/5][500/1797] Data 0.006 (0.007) Batch 0.543 (0.544) Remain 00:28:03 MainLoss 0.7234 AuxLoss 0.8187 Loss 1.0509 Accuracy 0.7342.
[2025-02-11 06:31:31,770] INFO: Epoch: [4/5][600/1797] Data 0.006 (0.006) Batch 0.544 (0.544) Remain 00:27:08 MainLoss 0.4872 AuxLoss 0.9034 Loss 0.8485 Accuracy 0.9040.
[2025-02-11 06:32:26,071] INFO: Epoch: [4/5][700/1797] Data 0.006 (0.006) Batch 0.542 (0.544) Remain 00:26:13 MainLoss 0.6175 AuxLoss 0.6443 Loss 0.8753 Accuracy 0.7753.
[2025-02-11 06:33:20,344] INFO: Epoch: [4/5][800/1797] Data 0.006 (0.006) Batch 0.542 (0.544) Remain 00:25:18 MainLoss 0.5461 AuxLoss 0.5545 Loss 0.7679 Accuracy 0.8443.
[2025-02-11 06:34:14,642] INFO: Epoch: [4/5][900/1797] Data 0.006 (0.006) Batch 0.543 (0.544) Remain 00:24:24 MainLoss 0.5679 AuxLoss 0.6777 Loss 0.8390 Accuracy 0.8275.
